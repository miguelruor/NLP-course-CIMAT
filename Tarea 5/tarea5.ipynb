{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45PSPma3uo6m"
      },
      "source": [
        "# Miguel Angel Ruiz Ortiz\n",
        "## Procesamiento de Lenguaje Natural\n",
        "## Tarea 5: Modelo de Lenguaje Neuronales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JQOkch9uUTu",
        "outputId": "9fd41237-310b-4db8-a956-15d52742b511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MYo8rA5avhIl"
      },
      "outputs": [],
      "source": [
        "from typing import Union, Optional\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "from pathlib import Path\n",
        "import time\n",
        "from itertools import permutations\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.util import ngrams\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGGB-ou9brGs"
      },
      "source": [
        "Creación de directorios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xR6hBomrbQKa"
      },
      "outputs": [],
      "source": [
        "base_path = Path(\"/content/drive/My Drive/Academic Stuff/NLP (CIMAT)\")\n",
        "\n",
        "# Saving directories\n",
        "savedir = base_path / \"models-tarea-5\" / \"from_pretrain\" # directory for model using pretrained embeddings\n",
        "os.makedirs(savedir, exist_ok = True)\n",
        "\n",
        "savedir_scratch = base_path / \"models-tarea-5\" / \"from_scratch\" # directory for model from scratch\n",
        "os.makedirs(savedir_scratch, exist_ok = True)\n",
        "\n",
        "savedir_char = base_path / \"models-tarea-5\" / \"characters\" # directory for characters model\n",
        "os.makedirs(savedir_char, exist_ok = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuIjJ5f-JcLo"
      },
      "source": [
        "# 1) Modelo de Lenguaje Neuronal (Bengio 2003) a nivel de palabra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIBb2XnCsmbg"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.1) Implementación del modelo\n",
        "\n",
        "Con base en la implementación mostrada en la práctica del modelo de Bengio, construya un modelo de lenguaje neuronal a nivel de palabra, pero preinicializado con los embeddings proporcionados. Tome en cuenta secuencias de tamaño 4 para el modelo, es decir hasta 3 palabras en el contexto.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz4aZlk2zGqx"
      },
      "source": [
        "En las siguientes celdas se encuentra el código para leer y procesar el corpus de \"MEX-A3T\". Se utiliza TweetTokenizer para tokenizar el texto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4FHpk_UVzID6"
      },
      "outputs": [],
      "source": [
        "def get_corpus(corpus_path: Path) -> list[str]:\n",
        "    with open(corpus_path, \"r\") as corpus_file:\n",
        "        corpus = [line for line in corpus_file]\n",
        "\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "77XJiLnrxbcr"
      },
      "outputs": [],
      "source": [
        "mex_a3t_path = base_path / \"corpus/MEX-A3T\"\n",
        "\n",
        "train_corpus_path = mex_a3t_path / \"mex20_train.txt\"\n",
        "val_corpus_path = mex_a3t_path / \"mex20_val.txt\"\n",
        "embeddings_path = mex_a3t_path / \"word2vec_col.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "82XxR1kDzOym"
      },
      "outputs": [],
      "source": [
        "train_corpus = get_corpus(train_corpus_path)\n",
        "val_corpus = get_corpus(val_corpus_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DqXv02KFzxhz"
      },
      "outputs": [],
      "source": [
        "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
        "\n",
        "train_corpus_tk = [tokenizer.tokenize(tweet) for tweet in train_corpus]\n",
        "val_corpus_tk = [tokenizer.tokenize(tweet) for tweet in val_corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5kYP0HVxX8-"
      },
      "source": [
        "Cargamos los embeddings iniciales:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BZdhUz4rv3rE"
      },
      "outputs": [],
      "source": [
        "with open(embeddings_path, \"r\") as file:\n",
        "    lines_embeddings_file = file.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s0Ny7gyq2vIF"
      },
      "outputs": [],
      "source": [
        "num_words, embedding_dim = map(int, lines_embeddings_file[0].split())\n",
        "word2embedding = {\n",
        "    line_splitted[0]: np.array(list(map(float, line_splitted[1:])))\n",
        "    for line_splitted in map(str.split, lines_embeddings_file[1:])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRhE3yhl21J0",
        "outputId": "1fc2778a-41c7-4eeb-e92e-725f217aa086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de palabras con embedding: 973265\n",
            "Dimensión de los embeddings: 100\n"
          ]
        }
      ],
      "source": [
        "print(\"Número de palabras con embedding:\", num_words)\n",
        "print(\"Dimensión de los embeddings:\", embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_e0XPJ35zqX"
      },
      "source": [
        "Ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfA4HB_H3_z7",
        "outputId": "2ebe4ec0-344b-4fc7-e85c-2b06cd15c6f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-1.419667, -0.490418, -1.444962,  0.864942, -2.474545, -2.819041,\n",
              "       -0.195111,  3.268535,  4.201846, -0.446295,  0.132508, -3.323097,\n",
              "       -1.335639,  2.86414 ,  0.775206, -2.351034,  3.294083, -2.585027,\n",
              "       -3.064607,  0.274417,  3.548857,  3.086329,  0.119739,  0.577198,\n",
              "       -1.788768,  2.477334, -1.746314,  0.747134,  2.337681, -4.256221,\n",
              "        3.570596, -0.41506 ,  1.456289,  0.148753, -4.042562, -1.551155,\n",
              "       -0.978901,  1.965899, -0.331655,  1.018842,  2.553949,  1.254084,\n",
              "       -0.789299,  2.823506, -5.736207, -0.169698,  1.530003,  3.976882,\n",
              "        0.497212,  0.294316,  1.58776 , -2.974533, -0.832896, -0.161019,\n",
              "       -1.31667 , -2.505708,  1.711155, -0.819489,  0.6929  , -6.522143,\n",
              "       -2.402351,  3.085217, -1.504392,  0.314337,  1.760254,  0.297669,\n",
              "        0.689544, -0.704122, -3.248115,  0.832989, -0.923742, -0.966281,\n",
              "        0.48139 , -5.741403, -2.064541,  1.670688, -4.450252,  0.124791,\n",
              "        0.393129,  1.819823,  0.462336,  2.106388,  1.315617,  3.42831 ,\n",
              "       -1.429179, -2.808474, -4.349866,  2.703398, -1.626008,  3.358473,\n",
              "       -0.836135,  3.53257 ,  2.912096, -1.236164, -3.193108, -1.709333,\n",
              "        0.556389, -3.627885, -6.647338, -1.112334])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2embedding[\"hola\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4zIdvak-JBt"
      },
      "source": [
        "A continuación veremos la cantidad de palabras en el conjunto de entrenamiento y cuántas de ellas tienen un embedding preentrenado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a6Njy7P4wK_",
        "outputId": "3c12f29e-8da1-4b8e-ce78-6712ac794f4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de palabras en el conjunto de entrenamiento: 13071\n"
          ]
        }
      ],
      "source": [
        "train_words = set()\n",
        "for tweet in train_corpus_tk:\n",
        "    train_words.update(tweet)\n",
        "\n",
        "print(\"Número de palabras en el conjunto de entrenamiento:\",  len(train_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW8DYKuB5MUF",
        "outputId": "d4de6514-8adf-488f-f073-b5a8749343a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de palabras en el conjunto de entrenamiento que también tienen un embedding: 11425\n"
          ]
        }
      ],
      "source": [
        "embedddings_words = set(word2embedding.keys())\n",
        "\n",
        "print(\"Número de palabras en el conjunto de entrenamiento que también tienen un embedding:\", len(train_words.intersection(embedddings_words)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oxHi4lE-3FR"
      },
      "source": [
        "El vocabulario que vamos a considerar constará justo de aquellas palabras en nuestro conjunto de entrenamiento que también tienen un embedding preentrenado. Además, vamos a agregar los tokens especiales ``<s>``, ``</s>`` y ``<unk>`` al vocabulario con un embedding aleatorio inicial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "h6EdkXD2BUCH"
      },
      "outputs": [],
      "source": [
        "# special tokens\n",
        "INIT_TKN = \"<s>\"\n",
        "END_TKN = \"</s>\"\n",
        "UNK_TKN = \"<unk>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7K5NPQrIAe51"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "vocab = train_words.intersection(embedddings_words)\n",
        "embeddings = { word: word2embedding[word] for word in vocab }\n",
        "\n",
        "vocab.update([INIT_TKN, END_TKN, UNK_TKN])\n",
        "vocab_len = len(vocab)\n",
        "\n",
        "embeddings[INIT_TKN] = np.random.rand(embedding_dim)\n",
        "embeddings[END_TKN] = np.random.rand(embedding_dim)\n",
        "embeddings[UNK_TKN] = np.random.rand(embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVQkMgHWBwR-"
      },
      "source": [
        "Reutilizamos la clase de la tarea pasada para procesar el texto (quitar signos de puntuación, crear mapeos ``{palabra -> id}`` (y viceversa), y enmascaras palabras fuera del vocabulario) con unas modicaciones para que considere un vocabulario dado. Puede recibir de manera opcional un mapeo ``{palabra -> id}`` precalculado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vQtDOKOp_DxS"
      },
      "outputs": [],
      "source": [
        "class TextProcessor:\n",
        "    def __init__(self, vocab: set, word2id: Optional[dict[str, int]] = None):\n",
        "        # special tokens. we assume they are already present in the vocabulary given\n",
        "        self.INIT_TKN = \"<s>\"\n",
        "        self.END_TKN = \"</s>\"\n",
        "        self.UNK_TKN = \"<unk>\"\n",
        "\n",
        "        # punctuation signs that will be not considered\n",
        "        self.punctuation = {\"¡\", \"!\", '\"', \"$\", \"%\", \"&\", \"'\", \"(\", \")\", \"*\", \"+\", \",\", \"-\", \".\", \":\", \";\", \"¿\", \"?\", \"@\", \"[\", \"]\", \"_\", \"`\", \"{\", \"}\", \"«\", \"»\", \"…\"}\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.vocab_len = len(vocab)\n",
        "\n",
        "        if word2id is None:\n",
        "            self.word2id = { word: idx for idx, word in enumerate(vocab) } # mapping {word -> word id}\n",
        "        else:\n",
        "            self.word2id = word2id\n",
        "\n",
        "        self.id2word = { idx: word for word, idx in self.word2id.items() } # mapping {word id -> word}\n",
        "\n",
        "    def mask_oov(self, text: Union[str, list[str]]) -> Union[str, list[str]]:\n",
        "        \"\"\"Replace out-of-vocabulary words with <unk>\"\"\"\n",
        "        if isinstance(text, str):\n",
        "            return text if text in self.vocab else self.UNK_TKN\n",
        "        else:\n",
        "            # than it is list of strings\n",
        "            return [self.mask_oov(word) for word in text if word not in self.punctuation]\n",
        "\n",
        "    def mask_text(self, text: list[str]) -> list[str]:\n",
        "        \"\"\" Mask if out of vocabulary and add initial and end sentence\n",
        "        \"\"\"\n",
        "        return [self.INIT_TKN] + self.mask_oov(text) + [self.END_TKN]\n",
        "\n",
        "    def transform(self, corpus: list[list[str]]) -> list[list[str]]:\n",
        "        return [self.mask_text(text) for text in corpus]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3KOJDloZr2B"
      },
      "source": [
        "Notemos que el diccionario ``word2id`` en la clase ``TextProcessor`` depende de la iteración sobre el ``set`` ``vocab`` (el vocabulario). Resulta que Python no asegura ningún orden en la manera que se itera un ``set``, de tal manera que en diferentes sesiones de Python el mapeo {palabra -> id} puede cambiar (comprobado durante la realización de esta tarea). Si uno quiere cargar el mejor modelo entrenado en una sesión anterior, puede que el orden en que se guarda la matriz de embeddings no corresponda con el nuevo mapeo {palabra -> id} calculado en la sesión actual. Así que vamos a guardar ese mapeo en un archivo json."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XBZG9Em7bcPF"
      },
      "outputs": [],
      "source": [
        "word2id_path = base_path / \"models-tarea-5\" / \"word2id.json\"\n",
        "\n",
        "if word2id_path.exists():\n",
        "    with open(word2id_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        word2id_json = json.load(file)\n",
        "else:\n",
        "    word2id_json = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aZwAtIpWK3OT"
      },
      "outputs": [],
      "source": [
        "processor = TextProcessor(vocab=vocab, word2id=word2id_json)\n",
        "train_corpus_msk = processor.transform(train_corpus_tk)\n",
        "val_corpus_msk = processor.transform(val_corpus_tk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TjgeCxQdMER"
      },
      "source": [
        "Si no habíamos creado el archivo json antes, lo hacemos con el mapeo calculado en ``processor``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EVNVhquLdFE6"
      },
      "outputs": [],
      "source": [
        "if word2id_json is None:\n",
        "    with open(word2id_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        json.dump(processor.word2id, file, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TsQE5fiGQXU"
      },
      "source": [
        "Porcentaje de tokens ``<UNK>`` en el corpus de entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjqsCPd8FWT3",
        "outputId": "f9be97f4-73bc-466e-b8e6-4e21831b1015"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5.585791741398149"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(\n",
        "    sum([1 if tkn == UNK_TKN else 0 for tkn in tweet]) for tweet in train_corpus_msk\n",
        ") / sum(len(tweet) for tweet in train_corpus_msk) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3Mt3JsypVvv"
      },
      "source": [
        "Con el mapeo ``{ palabra -> id }`` obtenido en el objeto ``processor``, obtenemos la matriz de embeddings preentrenados asociada a nuestro vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7cMv4C4rplRj"
      },
      "outputs": [],
      "source": [
        "embeddings_w = np.array([embeddings[processor.id2word[i]] for i in range(vocab_len)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJPWZFSTpxiX",
        "outputId": "5692373b-d615-4c2f-c804-2f491081a7d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11428, 100)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_w.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_YSrhzLQFBe"
      },
      "source": [
        "Con la siguiente función generamos el conjunto de entrenamiento y validación con los n-gramas (n=4) de los corpus correspondientes  (3 palabras de contexto y una de predicción). La siguiente función recibe un objeto ``TextProcessor`` y un corpus previamente enmascarado con ``TextProcessor``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NO0UGx9QPEjk"
      },
      "outputs": [],
      "source": [
        "def get_ngrams(masked_corpus: list[list[str]], n: int, text_processor: TextProcessor) -> tuple[np.ndarray, np.ndarray]:\n",
        "\n",
        "    X_ngrams = []\n",
        "    y = []\n",
        "\n",
        "    for doc in masked_corpus:\n",
        "        # we assume doc has only one initial token <s> and one end token </s>, added in TextProcessor\n",
        "        doc_pad = [text_processor.INIT_TKN]*(n-2) + doc\n",
        "\n",
        "        for ngram in ngrams(doc_pad, n):\n",
        "            ngram_ids = [text_processor.word2id[w] for w in ngram]\n",
        "            X_ngrams.append(ngram_ids[:-1])\n",
        "            y.append(ngram_ids[-1])\n",
        "\n",
        "    return np.array(X_ngrams), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7uKAHyryRYJz"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = get_ngrams(masked_corpus=train_corpus_msk, n=4, text_processor=processor)\n",
        "X_val, y_val = get_ngrams(masked_corpus=val_corpus_msk, n=4, text_processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yceHqx7HRzb5",
        "outputId": "920a22aa-b5af-45e3-bed5-3907d0e293a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((91736, 3), (91736,), (10319, 3), (10319,))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMa3NUZHSCzy",
        "outputId": "6da41a99-fe5d-4325-c8c6-94424f68f5eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['<s>', '<s>', '<s>'],\n",
              " ['<s>', '<s>', 'q'],\n",
              " ['<s>', 'q', 'se'],\n",
              " ['q', 'se', 'puede'],\n",
              " ['se', 'puede', 'esperar'],\n",
              " ['puede', 'esperar', 'del'],\n",
              " ['esperar', 'del', 'maricon'],\n",
              " ['del', 'maricon', 'de'],\n",
              " ['maricon', 'de', 'closet'],\n",
              " ['de', 'closet', 'de']]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[[processor.id2word[w] for w in tw] for tw in X_train[:10]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3qNR-9-Shf5"
      },
      "source": [
        "Definimos los objetos ``TensorDataset`` y ``DataLoader`` con nuestros datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cpv2C8ytS0U7"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# training\n",
        "train_dataset = TensorDataset(\n",
        "    torch.tensor(X_train, dtype=torch.int64), torch.tensor(y_train, dtype=torch.int64)\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True\n",
        ")\n",
        "\n",
        "# validation\n",
        "val_dataset = TensorDataset(\n",
        "    torch.tensor(X_val, dtype=torch.int64), torch.tensor(y_val, dtype=torch.int64)\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPHDzlrYX3G7"
      },
      "source": [
        "La siguiente clase implementa el modelo de lenguaje neuronal de Bengio. El código es el del profesor de la práctica 4, pero adaptado para utilizar embeddings preentrenados y con *type hints*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "j7PculvkX7Ss"
      },
      "outputs": [],
      "source": [
        "class NeuralLM(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        window_size: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_dim: int,\n",
        "        vocab_size: int,\n",
        "        pretrained_embeddings: Optional[np.ndarray] = None,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.window_size = window_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.emb = nn.Embedding.from_pretrained(\n",
        "                torch.tensor(pretrained_embeddings, dtype=torch.float),\n",
        "                freeze=False,\n",
        "            )\n",
        "        else:\n",
        "            self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.dense_1 = nn.Linear(embedding_dim * (window_size), hidden_dim)\n",
        "        self.drop1 = nn.Dropout(p=dropout)\n",
        "        self.dense_2 = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        x = self.emb(x)\n",
        "        x = x.view(-1, self.window_size * self.embedding_dim)\n",
        "        h = nn.functional.relu(self.dense_1(x))\n",
        "        h = self.drop1(h)\n",
        "        return self.dense_2(h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz0NSyl4fJND"
      },
      "source": [
        "Las siguientes funciones también son del código del profesor, las cuales sirven para obtener la predicción del modelo, evaluar el modelo y guardar checkpoints del modelo durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "14qXyHfpZLhB"
      },
      "outputs": [],
      "source": [
        "def get_preds(raw_logits: torch.FloatTensor) -> np.ndarray:\n",
        "    probs = nn.functional.softmax(raw_logits.detach(), dim=1)\n",
        "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "def model_eval(data: DataLoader, model: NeuralLM, gpu: bool = False) -> float:\n",
        "    with torch.no_grad():\n",
        "        preds, tgts = [], []\n",
        "        for window_words, labels in data:\n",
        "            if gpu:\n",
        "                window_words = window_words.cuda()\n",
        "\n",
        "            outputs = model(window_words)\n",
        "\n",
        "            # Get prediction\n",
        "            y_pred = get_preds(outputs)\n",
        "\n",
        "            tgt = labels.numpy()\n",
        "            tgts.append(tgt)\n",
        "            preds.append(y_pred)\n",
        "\n",
        "    tgts = [e for l in tgts for e in l]\n",
        "    preds = [e for l in preds for e in l]\n",
        "\n",
        "    return accuracy_score(tgts, preds)\n",
        "\n",
        "def save_checkpoint(state: dict, is_best: bool, checkpoint_path: Path):\n",
        "    filename = checkpoint_path / \"checkpoint.pt\"\n",
        "    torch.save(state, filename)\n",
        "\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, checkpoint_path / \"model_best.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-g3363cfkLG"
      },
      "source": [
        "Hiperparámetros del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "gqvRnY08adJD"
      },
      "outputs": [],
      "source": [
        "# Model hyperparameters\n",
        "embedding_dim = 100  # Dimension of word embedding\n",
        "hidden_dim = 200  # Dimension for hidden layer\n",
        "dropout = 0.2\n",
        "\n",
        "# Training hyperparameters\n",
        "lr = 2.3e-1\n",
        "num_epochs = 100\n",
        "patience = 20\n",
        "\n",
        "# Scheduler hyperparameters\n",
        "lr_patience = 10\n",
        "lr_factor = 0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s7PZhM7fnSD"
      },
      "source": [
        "Definición del modelo, función de pérdida, optimizador y scheduler para actualizar el *learning rate*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFkSq-M4dzmP"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = NeuralLM(\n",
        "    window_size = 3,\n",
        "    embedding_dim = embedding_dim,\n",
        "    hidden_dim = hidden_dim,\n",
        "    vocab_size = vocab_len,\n",
        "    pretrained_embeddings = embeddings_w,\n",
        "    dropout = dropout\n",
        ")\n",
        "\n",
        "# Send to GPU\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "# Loss, Optimizer and Scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer, \"min\",\n",
        "                patience = lr_patience,\n",
        "                factor = lr_factor\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13mLxlpicmkM"
      },
      "source": [
        "Entrenamiento del modelo (código del profesor):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FWJRXRicj6Y",
        "outputId": "2e74d591-c8b8-4216-e66e-d92eb608330b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train acc: 0.09933243258949327\n",
            "Epoch [1/100], Loss: 6.0868 - Val accuracy: 0.1119 - Epoch time: 12.04s\n",
            "---12.04413890838623 seconds ---\n",
            "Train acc: 0.10462793468154348\n",
            "Epoch [2/100], Loss: 5.8519 - Val accuracy: 0.1037 - Epoch time: 7.16s\n",
            "---19.204473972320557 seconds ---\n",
            "Train acc: 0.10935683984193399\n",
            "Epoch [3/100], Loss: 5.6523 - Val accuracy: 0.1052 - Epoch time: 5.98s\n",
            "---25.187881469726562 seconds ---\n",
            "Train acc: 0.11151789865178986\n",
            "Epoch [4/100], Loss: 5.4623 - Val accuracy: 0.1034 - Epoch time: 7.12s\n",
            "---32.30402064323425 seconds ---\n",
            "Train acc: 0.11458696536494654\n",
            "Epoch [5/100], Loss: 5.2962 - Val accuracy: 0.1166 - Epoch time: 6.30s\n",
            "---38.606375217437744 seconds ---\n",
            "Train acc: 0.11929407833565785\n",
            "Epoch [6/100], Loss: 5.1337 - Val accuracy: 0.1197 - Epoch time: 7.21s\n",
            "---45.812177419662476 seconds ---\n",
            "Train acc: 0.1230423349604835\n",
            "Epoch [7/100], Loss: 4.9877 - Val accuracy: 0.1114 - Epoch time: 6.10s\n",
            "---51.90981698036194 seconds ---\n",
            "Train acc: 0.12840321362157137\n",
            "Epoch [8/100], Loss: 4.8553 - Val accuracy: 0.1119 - Epoch time: 7.16s\n",
            "---59.07440423965454 seconds ---\n",
            "Train acc: 0.133491689911669\n",
            "Epoch [9/100], Loss: 4.7364 - Val accuracy: 0.1201 - Epoch time: 6.23s\n",
            "---65.30203080177307 seconds ---\n",
            "Train acc: 0.13922666782891677\n",
            "Epoch [10/100], Loss: 4.6358 - Val accuracy: 0.0977 - Epoch time: 7.02s\n",
            "---72.3231029510498 seconds ---\n",
            "Train acc: 0.14809608902835888\n",
            "Epoch [11/100], Loss: 4.5383 - Val accuracy: 0.1054 - Epoch time: 6.13s\n",
            "---78.4562075138092 seconds ---\n",
            "Train acc: 0.15353687238493724\n",
            "Epoch [12/100], Loss: 4.4500 - Val accuracy: 0.1140 - Epoch time: 6.96s\n",
            "---85.41188955307007 seconds ---\n",
            "Train acc: 0.16214841933984195\n",
            "Epoch [13/100], Loss: 4.3723 - Val accuracy: 0.0972 - Epoch time: 6.38s\n",
            "---91.79686713218689 seconds ---\n",
            "Train acc: 0.16710977452347744\n",
            "Epoch [14/100], Loss: 4.3074 - Val accuracy: 0.1087 - Epoch time: 7.30s\n",
            "---99.09606122970581 seconds ---\n",
            "Train acc: 0.17447190260344025\n",
            "Epoch [15/100], Loss: 4.2407 - Val accuracy: 0.1009 - Epoch time: 6.19s\n",
            "---105.28260135650635 seconds ---\n",
            "Train acc: 0.1812056892143189\n",
            "Epoch [16/100], Loss: 4.1888 - Val accuracy: 0.1186 - Epoch time: 7.08s\n",
            "---112.35900211334229 seconds ---\n",
            "Train acc: 0.18718401324965134\n",
            "Epoch [17/100], Loss: 4.1412 - Val accuracy: 0.1114 - Epoch time: 6.23s\n",
            "---118.58758807182312 seconds ---\n",
            "Train acc: 0.190449209669921\n",
            "Epoch [18/100], Loss: 4.1057 - Val accuracy: 0.0998 - Epoch time: 7.08s\n",
            "---125.66663599014282 seconds ---\n",
            "Train acc: 0.1993440550906555\n",
            "Epoch [19/100], Loss: 4.0444 - Val accuracy: 0.1145 - Epoch time: 6.13s\n",
            "---131.7959418296814 seconds ---\n",
            "Train acc: 0.20389135867038585\n",
            "Epoch [20/100], Loss: 4.0089 - Val accuracy: 0.1095 - Epoch time: 7.09s\n",
            "---138.8871099948883 seconds ---\n",
            "Train acc: 0.2084931427243143\n",
            "Epoch [21/100], Loss: 3.9737 - Val accuracy: 0.1050 - Epoch time: 6.23s\n",
            "---145.11829924583435 seconds ---\n",
            "Train acc: 0.21306587052533704\n",
            "Epoch [22/100], Loss: 3.9343 - Val accuracy: 0.1179 - Epoch time: 7.15s\n",
            "---152.26593327522278 seconds ---\n",
            "Train acc: 0.21622573802882378\n",
            "Epoch [23/100], Loss: 3.9037 - Val accuracy: 0.1166 - Epoch time: 6.29s\n",
            "---158.5578420162201 seconds ---\n",
            "Train acc: 0.21902240237099024\n",
            "Epoch [24/100], Loss: 3.8771 - Val accuracy: 0.1049 - Epoch time: 7.20s\n",
            "---165.7546489238739 seconds ---\n",
            "Train acc: 0.25465263249651326\n",
            "Epoch [25/100], Loss: 3.5346 - Val accuracy: 0.1139 - Epoch time: 6.26s\n",
            "---172.01251459121704 seconds ---\n",
            "Train acc: 0.26934420037192003\n",
            "Epoch [26/100], Loss: 3.4522 - Val accuracy: 0.1123 - Epoch time: 7.19s\n",
            "---179.20518016815186 seconds ---\n",
            "Train acc: 0.2698962691771269\n",
            "Epoch [27/100], Loss: 3.4256 - Val accuracy: 0.1200 - Epoch time: 6.38s\n",
            "---185.58177614212036 seconds ---\n",
            "Train acc: 0.2725185960018596\n",
            "Epoch [28/100], Loss: 3.4067 - Val accuracy: 0.1217 - Epoch time: 7.28s\n",
            "---192.85930800437927 seconds ---\n",
            "Train acc: 0.2733103788935379\n",
            "Epoch [29/100], Loss: 3.3954 - Val accuracy: 0.1175 - Epoch time: 6.21s\n",
            "---199.07069039344788 seconds ---\n",
            "Train acc: 0.2769859948860995\n",
            "Epoch [30/100], Loss: 3.3710 - Val accuracy: 0.1206 - Epoch time: 7.02s\n",
            "---206.09253549575806 seconds ---\n",
            "Train acc: 0.27852234425848443\n",
            "Epoch [31/100], Loss: 3.3589 - Val accuracy: 0.1171 - Epoch time: 6.06s\n",
            "---212.15356183052063 seconds ---\n",
            "Train acc: 0.2816313633193863\n",
            "Epoch [32/100], Loss: 3.3383 - Val accuracy: 0.1167 - Epoch time: 7.07s\n",
            "---219.22106981277466 seconds ---\n",
            "Train acc: 0.28361808461180843\n",
            "Epoch [33/100], Loss: 3.3239 - Val accuracy: 0.1197 - Epoch time: 6.08s\n",
            "---225.3035864830017 seconds ---\n",
            "Train acc: 0.2867525278940028\n",
            "Epoch [34/100], Loss: 3.3095 - Val accuracy: 0.1089 - Epoch time: 7.07s\n",
            "---232.36976170539856 seconds ---\n",
            "Train acc: 0.2886992968386797\n",
            "Epoch [35/100], Loss: 3.2931 - Val accuracy: 0.1131 - Epoch time: 6.39s\n",
            "---238.76342487335205 seconds ---\n",
            "Train acc: 0.3153111924686193\n",
            "Epoch [36/100], Loss: 3.0834 - Val accuracy: 0.1272 - Epoch time: 7.64s\n",
            "---246.39896202087402 seconds ---\n",
            "Train acc: 0.32103890632264065\n",
            "Epoch [37/100], Loss: 3.0432 - Val accuracy: 0.1200 - Epoch time: 6.29s\n",
            "---252.68734884262085 seconds ---\n",
            "Train acc: 0.3215510227801023\n",
            "Epoch [38/100], Loss: 3.0260 - Val accuracy: 0.1239 - Epoch time: 7.29s\n",
            "---259.98177909851074 seconds ---\n",
            "Train acc: 0.32279317759181775\n",
            "Epoch [39/100], Loss: 3.0150 - Val accuracy: 0.1296 - Epoch time: 6.21s\n",
            "---266.188752412796 seconds ---\n",
            "Train acc: 0.32863711645746163\n",
            "Epoch [40/100], Loss: 3.0036 - Val accuracy: 0.1228 - Epoch time: 7.33s\n",
            "---273.5171148777008 seconds ---\n",
            "Train acc: 0.3271988319386332\n",
            "Epoch [41/100], Loss: 2.9981 - Val accuracy: 0.1263 - Epoch time: 6.21s\n",
            "---279.72987842559814 seconds ---\n",
            "Train acc: 0.32838287424453744\n",
            "Epoch [42/100], Loss: 2.9842 - Val accuracy: 0.1257 - Epoch time: 7.16s\n",
            "---286.893758058548 seconds ---\n",
            "Train acc: 0.3300754009762901\n",
            "Epoch [43/100], Loss: 2.9768 - Val accuracy: 0.1268 - Epoch time: 6.28s\n",
            "---293.1735758781433 seconds ---\n",
            "Train acc: 0.33032601115760113\n",
            "Epoch [44/100], Loss: 2.9712 - Val accuracy: 0.1234 - Epoch time: 7.15s\n",
            "---300.32804894447327 seconds ---\n",
            "Train acc: 0.33293017782426776\n",
            "Epoch [45/100], Loss: 2.9568 - Val accuracy: 0.1253 - Epoch time: 6.26s\n",
            "---306.58503556251526 seconds ---\n",
            "Train acc: 0.3343176139005114\n",
            "Epoch [46/100], Loss: 2.9534 - Val accuracy: 0.1213 - Epoch time: 7.32s\n",
            "---313.9057242870331 seconds ---\n",
            "Train acc: 0.3531024814039982\n",
            "Epoch [47/100], Loss: 2.8143 - Val accuracy: 0.1321 - Epoch time: 6.46s\n",
            "---320.36459469795227 seconds ---\n",
            "Train acc: 0.35761709669920966\n",
            "Epoch [48/100], Loss: 2.7897 - Val accuracy: 0.1288 - Epoch time: 7.10s\n",
            "---327.4638123512268 seconds ---\n",
            "Train acc: 0.3567454091120409\n",
            "Epoch [49/100], Loss: 2.7871 - Val accuracy: 0.1278 - Epoch time: 6.20s\n",
            "---333.66451716423035 seconds ---\n",
            "Train acc: 0.3609985181311018\n",
            "Epoch [50/100], Loss: 2.7712 - Val accuracy: 0.1252 - Epoch time: 7.10s\n",
            "---340.7684602737427 seconds ---\n",
            "Train acc: 0.36126728847047884\n",
            "Epoch [51/100], Loss: 2.7688 - Val accuracy: 0.1275 - Epoch time: 6.21s\n",
            "---346.97921109199524 seconds ---\n",
            "Train acc: 0.36160506741050674\n",
            "Epoch [52/100], Loss: 2.7612 - Val accuracy: 0.1279 - Epoch time: 8.38s\n",
            "---355.3623218536377 seconds ---\n",
            "Train acc: 0.36274552533705257\n",
            "Epoch [53/100], Loss: 2.7561 - Val accuracy: 0.1253 - Epoch time: 6.21s\n",
            "---361.57214736938477 seconds ---\n",
            "Train acc: 0.3619355822873082\n",
            "Epoch [54/100], Loss: 2.7559 - Val accuracy: 0.1257 - Epoch time: 7.19s\n",
            "---368.7668106555939 seconds ---\n",
            "Train acc: 0.36310509646675965\n",
            "Epoch [55/100], Loss: 2.7451 - Val accuracy: 0.1327 - Epoch time: 6.33s\n",
            "---375.09824323654175 seconds ---\n",
            "Train acc: 0.36398768014876803\n",
            "Epoch [56/100], Loss: 2.7419 - Val accuracy: 0.1208 - Epoch time: 7.08s\n",
            "---382.17445158958435 seconds ---\n",
            "Train acc: 0.36466323802882383\n",
            "Epoch [57/100], Loss: 2.7368 - Val accuracy: 0.1278 - Epoch time: 6.35s\n",
            "---388.52386832237244 seconds ---\n",
            "Train acc: 0.3814432240818224\n",
            "Epoch [58/100], Loss: 2.6426 - Val accuracy: 0.1278 - Epoch time: 7.28s\n",
            "---395.8015959262848 seconds ---\n",
            "Train acc: 0.38318659925615994\n",
            "Epoch [59/100], Loss: 2.6279 - Val accuracy: 0.1278 - Epoch time: 6.17s\n",
            "---401.97279381752014 seconds ---\n",
            "Train acc: 0.3823984483960948\n",
            "Epoch [60/100], Loss: 2.6222 - Val accuracy: 0.1311 - Epoch time: 7.18s\n",
            "---409.1496732234955 seconds ---\n",
            "Train acc: 0.3843125290562529\n",
            "Epoch [61/100], Loss: 2.6150 - Val accuracy: 0.1300 - Epoch time: 6.29s\n",
            "---415.4421601295471 seconds ---\n",
            "Train acc: 0.3866261331938633\n",
            "Epoch [62/100], Loss: 2.6053 - Val accuracy: 0.1298 - Epoch time: 7.04s\n",
            "---422.4787108898163 seconds ---\n",
            "Train acc: 0.3854493549511855\n",
            "Epoch [63/100], Loss: 2.6073 - Val accuracy: 0.1318 - Epoch time: 6.21s\n",
            "---428.6860954761505 seconds ---\n",
            "Train acc: 0.38417451185495116\n",
            "Epoch [64/100], Loss: 2.6041 - Val accuracy: 0.1334 - Epoch time: 7.35s\n",
            "---436.03761172294617 seconds ---\n",
            "Train acc: 0.3866442933519293\n",
            "Epoch [65/100], Loss: 2.6010 - Val accuracy: 0.1300 - Epoch time: 10.42s\n",
            "---446.460200548172 seconds ---\n",
            "Train acc: 0.3854094026034403\n",
            "Epoch [66/100], Loss: 2.6004 - Val accuracy: 0.1305 - Epoch time: 6.73s\n",
            "---453.19145798683167 seconds ---\n",
            "Train acc: 0.38828233960948394\n",
            "Epoch [67/100], Loss: 2.5886 - Val accuracy: 0.1254 - Epoch time: 7.27s\n",
            "---460.457866191864 seconds ---\n",
            "Train acc: 0.3875595653184565\n",
            "Epoch [68/100], Loss: 2.5900 - Val accuracy: 0.1309 - Epoch time: 6.63s\n",
            "---467.0899736881256 seconds ---\n",
            "Train acc: 0.3965270513714551\n",
            "Epoch [69/100], Loss: 2.5310 - Val accuracy: 0.1306 - Epoch time: 7.25s\n",
            "---474.33863520622253 seconds ---\n",
            "Train acc: 0.3991203219432822\n",
            "Epoch [70/100], Loss: 2.5146 - Val accuracy: 0.1333 - Epoch time: 6.31s\n",
            "---480.6485767364502 seconds ---\n",
            "Train acc: 0.4016445839144584\n",
            "Epoch [71/100], Loss: 2.5104 - Val accuracy: 0.1297 - Epoch time: 7.13s\n",
            "---487.7773904800415 seconds ---\n",
            "Train acc: 0.40064940725244075\n",
            "Epoch [72/100], Loss: 2.5070 - Val accuracy: 0.1312 - Epoch time: 6.50s\n",
            "---494.27508664131165 seconds ---\n",
            "Train acc: 0.4029557473268247\n",
            "Epoch [73/100], Loss: 2.5024 - Val accuracy: 0.1318 - Epoch time: 7.30s\n",
            "---501.5752167701721 seconds ---\n",
            "Train acc: 0.40307197233844727\n",
            "Epoch [74/100], Loss: 2.5029 - Val accuracy: 0.1361 - Epoch time: 6.49s\n",
            "---508.0631535053253 seconds ---\n",
            "Train acc: 0.4019787308228731\n",
            "Epoch [75/100], Loss: 2.4970 - Val accuracy: 0.1312 - Epoch time: 7.35s\n",
            "---515.4178221225739 seconds ---\n",
            "Train acc: 0.40282136215713626\n",
            "Epoch [76/100], Loss: 2.4963 - Val accuracy: 0.1332 - Epoch time: 6.38s\n",
            "---521.7980539798737 seconds ---\n",
            "Train acc: 0.4032499418874942\n",
            "Epoch [77/100], Loss: 2.4931 - Val accuracy: 0.1342 - Epoch time: 7.31s\n",
            "---529.105546951294 seconds ---\n",
            "Train acc: 0.4029448512319851\n",
            "Epoch [78/100], Loss: 2.4912 - Val accuracy: 0.1363 - Epoch time: 6.19s\n",
            "---535.2979123592377 seconds ---\n",
            "Train acc: 0.4030283879590888\n",
            "Epoch [79/100], Loss: 2.4926 - Val accuracy: 0.1314 - Epoch time: 7.97s\n",
            "---543.2688431739807 seconds ---\n",
            "Train acc: 0.4146073047419805\n",
            "Epoch [80/100], Loss: 2.4448 - Val accuracy: 0.1286 - Epoch time: 6.24s\n",
            "---549.5091924667358 seconds ---\n",
            "Train acc: 0.41410971641097166\n",
            "Epoch [81/100], Loss: 2.4384 - Val accuracy: 0.1357 - Epoch time: 7.20s\n",
            "---556.7073419094086 seconds ---\n",
            "Train acc: 0.4140879242212924\n",
            "Epoch [82/100], Loss: 2.4381 - Val accuracy: 0.1347 - Epoch time: 6.32s\n",
            "---563.0253665447235 seconds ---\n",
            "Train acc: 0.4140733960948396\n",
            "Epoch [83/100], Loss: 2.4384 - Val accuracy: 0.1371 - Epoch time: 7.30s\n",
            "---570.3239288330078 seconds ---\n",
            "Train acc: 0.41575139470013944\n",
            "Epoch [84/100], Loss: 2.4301 - Val accuracy: 0.1344 - Epoch time: 6.23s\n",
            "---576.5587904453278 seconds ---\n",
            "Train acc: 0.4164814330543933\n",
            "Epoch [85/100], Loss: 2.4270 - Val accuracy: 0.1331 - Epoch time: 7.17s\n",
            "---583.7248940467834 seconds ---\n",
            "Train acc: 0.4134123663412367\n",
            "Epoch [86/100], Loss: 2.4260 - Val accuracy: 0.1356 - Epoch time: 6.05s\n",
            "---589.7750644683838 seconds ---\n",
            "Train acc: 0.4140334437470944\n",
            "Epoch [87/100], Loss: 2.4259 - Val accuracy: 0.1386 - Epoch time: 7.15s\n",
            "---596.9221823215485 seconds ---\n",
            "Train acc: 0.4158603556485356\n",
            "Epoch [88/100], Loss: 2.4252 - Val accuracy: 0.1317 - Epoch time: 6.21s\n",
            "---603.1283540725708 seconds ---\n",
            "Train acc: 0.41612549395629944\n",
            "Epoch [89/100], Loss: 2.4237 - Val accuracy: 0.1342 - Epoch time: 7.15s\n",
            "---610.2806515693665 seconds ---\n",
            "Train acc: 0.41565696187819623\n",
            "Epoch [90/100], Loss: 2.4267 - Val accuracy: 0.1347 - Epoch time: 6.15s\n",
            "---616.4355447292328 seconds ---\n",
            "Train acc: 0.42344040562529056\n",
            "Epoch [91/100], Loss: 2.3893 - Val accuracy: 0.1360 - Epoch time: 7.23s\n",
            "---623.6650495529175 seconds ---\n",
            "Train acc: 0.4241413877266388\n",
            "Epoch [92/100], Loss: 2.3806 - Val accuracy: 0.1354 - Epoch time: 6.23s\n",
            "---629.8946182727814 seconds ---\n",
            "Train acc: 0.4236583275220827\n",
            "Epoch [93/100], Loss: 2.3781 - Val accuracy: 0.1336 - Epoch time: 7.21s\n",
            "---637.1057136058807 seconds ---\n",
            "Train acc: 0.4243011971176197\n",
            "Epoch [94/100], Loss: 2.3786 - Val accuracy: 0.1375 - Epoch time: 6.32s\n",
            "---643.4229571819305 seconds ---\n",
            "Train acc: 0.42520194095769415\n",
            "Epoch [95/100], Loss: 2.3785 - Val accuracy: 0.1354 - Epoch time: 8.12s\n",
            "---651.5430045127869 seconds ---\n",
            "Train acc: 0.4228193282194328\n",
            "Epoch [96/100], Loss: 2.3791 - Val accuracy: 0.1389 - Epoch time: 6.32s\n",
            "---657.8640775680542 seconds ---\n",
            "Train acc: 0.4242612447698745\n",
            "Epoch [97/100], Loss: 2.3779 - Val accuracy: 0.1356 - Epoch time: 7.14s\n",
            "---665.0024244785309 seconds ---\n",
            "Train acc: 0.4247479370060437\n",
            "Epoch [98/100], Loss: 2.3768 - Val accuracy: 0.1368 - Epoch time: 6.32s\n",
            "---671.3226730823517 seconds ---\n",
            "Train acc: 0.4253871745699675\n",
            "Epoch [99/100], Loss: 2.3726 - Val accuracy: 0.1328 - Epoch time: 7.09s\n",
            "---678.4146876335144 seconds ---\n",
            "Train acc: 0.4253181659693166\n",
            "Epoch [100/100], Loss: 2.3742 - Val accuracy: 0.1349 - Epoch time: 6.23s\n",
            "---684.6427960395813 seconds ---\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "best_metric = 0\n",
        "metric_history = []\n",
        "train_metric_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    loss_epoch = []\n",
        "    training_metric = []\n",
        "    model.train()\n",
        "\n",
        "    for window_words, labels in train_loader:\n",
        "\n",
        "        # If GPU available\n",
        "        if use_gpu:\n",
        "            window_words = window_words.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(window_words)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_epoch.append(loss.item())\n",
        "\n",
        "        # Get training metrics\n",
        "        y_pred = get_preds(outputs)\n",
        "        tgt = labels.cpu().numpy()\n",
        "        training_metric.append(accuracy_score(tgt, y_pred))\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Get metric in validation dataset\n",
        "    mean_epoch_metric = np.mean(training_metric)\n",
        "    train_metric_history.append(mean_epoch_metric)\n",
        "\n",
        "    # Get metric in validation dataset\n",
        "    model.eval()\n",
        "    tuning_metric = model_eval(val_loader, model, gpu = use_gpu)\n",
        "    metric_history.append(mean_epoch_metric)\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step(tuning_metric)\n",
        "\n",
        "    # Check for metric improvement\n",
        "    is_improvement = tuning_metric > best_metric\n",
        "    if is_improvement:\n",
        "        best_metric = tuning_metric\n",
        "        n_no_improve = 0\n",
        "    else:\n",
        "        n_no_improve += 1\n",
        "\n",
        "    # Save best model if metric improved\n",
        "    save_checkpoint(\n",
        "      {\n",
        "        'epoch': epoch + 1,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict(),\n",
        "        'best_metric': best_metric,\n",
        "      },\n",
        "      is_improvement,\n",
        "      savedir\n",
        "    )\n",
        "\n",
        "    if n_no_improve >= patience:\n",
        "        print(\"No improvement. Breaking out of loop\")\n",
        "        break\n",
        "\n",
        "    print('Train acc: {}'.format(mean_epoch_metric))\n",
        "    print('Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}s'.format(\n",
        "        epoch + 1,\n",
        "        num_epochs,\n",
        "        np.mean(loss_epoch),\n",
        "        tuning_metric,\n",
        "        time.time() - epoch_start_time\n",
        "    ))\n",
        "\n",
        "    print(\"---%s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek0eM56QP_Yd"
      },
      "source": [
        "Cargamos el mejor modelo obtenido durante el entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggW7V6URQCXz",
        "outputId": "354ef908-a9c2-4756-f454-7a061d037567"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NeuralLM(\n",
              "  (emb): Embedding(11428, 100)\n",
              "  (dense_1): Linear(in_features=300, out_features=200, bias=True)\n",
              "  (drop1): Dropout(p=0.2, inplace=False)\n",
              "  (dense_2): Linear(in_features=200, out_features=11428, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model = NeuralLM(\n",
        "    window_size = 3,\n",
        "    embedding_dim = embedding_dim,\n",
        "    hidden_dim = hidden_dim,\n",
        "    vocab_size = vocab_len,\n",
        "    dropout = dropout\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "best_model.load_state_dict(torch.load(savedir / \"model_best.pt\", map_location=device)[\"state_dict\"])\n",
        "best_model.train(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHvtdRNYviaE"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.2) Similitud de palabras\n",
        "\n",
        "Después de haber entrenado el modelo, recupere las n palabras más similares a tres palabras de su gusto dadas.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMZruDZZOj2G"
      },
      "source": [
        "Función que regresa las 10 palabras más similares a una dada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "DB9yUl7OOnl8"
      },
      "outputs": [],
      "source": [
        "def n_similar_words(word: str, n: int, model: NeuralLM, processor: TextProcessor) -> list[tuple[str, float]]:\n",
        "    word_id = processor.word2id[word]\n",
        "    word_emb = model.emb(torch.LongTensor([word_id]))\n",
        "    dists = torch.linalg.norm(model.emb.weight - word_emb, dim=1).detach()\n",
        "    dists_ord = sorted(enumerate(dists), key=lambda x: x[1])\n",
        "\n",
        "    return [(processor.id2word[idx], dist) for idx, dist in dists_ord[1:n+1]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJDV-4FDd60p"
      },
      "source": [
        "Vamos a considerar las 10 palabras más similares a \"madre\", \"cabrón\", y \"chingada\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH_h9p_UOFqW",
        "outputId": "8d1a969d-5ce2-4705-90b9-30f0ab0d55b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras similares a madre :\n",
            "mama (15.2459), hermana (15.9248), abuela (15.9911), hija (17.1942), mamá (17.3497), abuelita (17.5991), vecina (18.3809), papá (18.4850), mamà (18.7762), padre (19.0859)\n",
            "------------------------------\n",
            "Palabras similares a cabrón :\n",
            "mamón (10.6598), cabron (11.3008), maricón (12.0740), culero (12.2599), marica (12.6395), putito (12.6703), imbécil (12.9490), baboso (13.1617), pendejo (13.3371), desgraciado (13.3585)\n",
            "------------------------------\n",
            "Palabras similares a chingada :\n",
            "fregada (11.1419), verga (15.3635), verch (16.5052), gaver (16.7693), vrg (16.7979), reputa (18.2148), vg (18.3495), putisima (18.5655), friendzone (18.6779), reputisima (18.7386)\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "words = [\"madre\", \"cabrón\", \"chingada\"]\n",
        "\n",
        "for word in words:\n",
        "    print(\"Palabras similares a\", word, \":\")\n",
        "    close_words = n_similar_words(word, 10, best_model, processor)\n",
        "    print(\", \".join([f\"{tup[0]} ({tup[1]:.4f})\" for tup in close_words]))\n",
        "    print(\"-\"*30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGGKv57ztFwg"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.3) Generación de texto\n",
        "\n",
        "Ponga al modelo a generar texto a partir de tres secuencias de inicio de su gusto.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aULsoshpjPYJ"
      },
      "source": [
        "La siguiente función genera texto utilizando el escalamiento con el parámetro de *temperatura* que utilizó el profesor en la práctica, pero después hacer el escalado se consideran sólo el top-k tokens con mayor probabilidad para predecir la siguiente palabra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "mgT2DYI6UawR"
      },
      "outputs": [],
      "source": [
        "def generate_text(\n",
        "    model: NeuralLM,\n",
        "    processor: TextProcessor,\n",
        "    seed: Optional[list[str]] = None,\n",
        "    n_tokens: int = 50,\n",
        "    top_k : int = 1000,\n",
        "    temperature: float = 1.0,\n",
        ") -> str:\n",
        "    if seed is None:\n",
        "        text = [processor.INIT_TKN] * (model.window_size)\n",
        "    else:\n",
        "        if len(seed) != model.window_size:\n",
        "            raise ValueError(\"seed should be of length of the window size of the model\")\n",
        "\n",
        "        text = list(map(processor.mask_oov, seed))\n",
        "\n",
        "\n",
        "    for i in range(n_tokens):\n",
        "        context = text[-model.window_size :]  # last tokens for the window\n",
        "\n",
        "        logits = model(torch.LongTensor([[processor.word2id[w] for w in context]])).detach().numpy()[0]\n",
        "        logits_adj = logits/temperature\n",
        "\n",
        "        # softmax\n",
        "        probs = np.exp(logits_adj)\n",
        "        probs = probs / np.sum(probs)\n",
        "\n",
        "        word_and_probs = [(processor.id2word[i], p) for i, p in enumerate(probs)]\n",
        "        word_and_probs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        words_topk = [w for w, p in word_and_probs[:top_k]]\n",
        "        probs_topk = np.array([p for w, p in word_and_probs[:top_k]])\n",
        "        probs_topk = probs_topk / np.sum(probs_topk) # normalize probabilities\n",
        "\n",
        "        pred_word = np.random.choice(words_topk, p=probs_topk)\n",
        "\n",
        "        text.append(pred_word)\n",
        "\n",
        "        if pred_word == processor.END_TKN:\n",
        "            break\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phjEYw1CmCee"
      },
      "source": [
        "Ejemplos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuU7OEau0I77",
        "outputId": "727861de-63da-4b18-c6ab-7743551c9d5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> <s> <s> es más fácil hacerla para que putas estas fotos y te valgo verga la vida de estos nacos que hace como las putas de colosio no se da y la madre teresa de calcuta </s>\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "sequence_1 = [\"<s>\", \"<s>\", \"<s>\"]\n",
        "print(\" \".join(generate_text(best_model, processor, seed=sequence_1, temperature=0.7, top_k=500)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MICVtYB90kwc",
        "outputId": "37a8d409-3b84-4467-a008-099ff5ed9b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> hola como la puta madre ya no son como tonta la pregunta para que se están mamando con coco de toronja verga ya clima lo me la pela la me vale verga </s>\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "sequence_2 = [\"<s>\", \"hola\", \"como\"]\n",
        "print(\" \".join(generate_text(best_model, processor, seed=sequence_2, temperature=0.8, top_k=500)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zyH_DLk0ldW",
        "outputId": "78cf27cf-6132-4173-ced3-933fb15e3404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hijo de la vergueishon tonatiuh cervantes explicó sobre las próximas inversiones para el estadio por la pero <unk> verga </s>\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(7)\n",
        "sequence_3 = [\"hijo\", \"de\", \"la\"]\n",
        "print(\" \".join(generate_text(best_model, processor, seed=sequence_3, temperature=0.8, top_k=500)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b66o04bjHQRH"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.4) Verosimilitud de oraciones\n",
        "\n",
        "Escriba 5 ejemplos de oraciones y mídales el likelihood.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqvpkIzmFiX"
      },
      "source": [
        "Función que calcula la log-verosimilitud de un texto dado (tokenizado), adaptada del código del profesor. Se prefiere la log-verosimilitud sobre la verosimilitud ya que la verosimilitud es un producto de probabilidades, de tal manera que se vuelve un número muy pequeño conforme aumenta el número de factores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NEjSowNq39v_"
      },
      "outputs": [],
      "source": [
        "def log_likelihood(model: NeuralLM, text: list[str], processor: TextProcessor) -> float:\n",
        "    text = processor.mask_text(text)\n",
        "\n",
        "    X, y = get_ngrams(masked_corpus=[text], n=4, text_processor=processor)\n",
        "    X, y = X[2:], y[2:]\n",
        "    X = torch.LongTensor(X)\n",
        "\n",
        "    logits = model(X).detach()\n",
        "    probs = nn.functional.softmax(logits, dim=1).numpy()\n",
        "\n",
        "    # consider the case when the probability is practically 0\n",
        "    return sum(np.log(probs[i][w] if not np.isclose(probs[i][w], 0) else 1e-8) for  i, w in enumerate(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97Q-Yd40mfr1"
      },
      "source": [
        "Ejemplos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paKiR8S66Evp",
        "outputId": "85f17715-c61f-4e31-d7f8-67f8d3817dca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "por eso estamos como estamos\n",
            "Log-verosimilitud: -42.60297\n",
            "------------------------------\n",
            "hola buen dia como están\n",
            "Log-verosimilitud: -57.450253\n",
            "------------------------------\n",
            "hijos de la chingada como creen\n",
            "Log-verosimilitud: -29.718723\n",
            "------------------------------\n",
            "vas a ver hijo de tu\n",
            "Log-verosimilitud: -28.189758\n",
            "------------------------------\n",
            "maldito clima hace un chingo de calor\n",
            "Log-verosimilitud: -45.35263\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    [\"por\", \"eso\", \"estamos\", \"como\", \"estamos\"],\n",
        "    [\"hola\", \"buen\", \"dia\", \"como\", \"están\"],\n",
        "    [\"hijos\", \"de\", \"la\", \"chingada\", \"como\", \"creen\"],\n",
        "    [\"vas\", \"a\", \"ver\", \"hijo\", \"de\", \"tu\"],\n",
        "    [\"maldito\", \"clima\", \"hace\", \"un\", \"chingo\", \"de\", \"calor\"]\n",
        "]\n",
        "for sent in sentences:\n",
        "    result = log_likelihood(best_model, sent, processor)\n",
        "    print(\" \".join(sent))\n",
        "    print(\"Log-verosimilitud:\", result)\n",
        "    print(\"-\"*30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWK4gVr4HTQy"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.5) Estructuras sintácticas\n",
        "\n",
        "Proponga un ejemplo para ver estructuras sintácticas (permutaciones de palabras de alguna oración) buenas usando el likelihood a partir de una oración que usted proponga.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1HimouWmng7"
      },
      "source": [
        "Dada una oración, calculamos la log-verosimilitud de todas las permutaciones de los tokens en la oración. Después se muestran las 5 permutaciones de mayor log-verosimilitud, y por lo tanto de mayor verosimilitud, i.e., las que tienen mayor estructura sintáctica. También se muestran las 5 permutaciones de menor verosimilitud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "NTFlKpry8H4w"
      },
      "outputs": [],
      "source": [
        "sentence = [\"hijos\", \"de\", \"la\", \"chingada\", \"van\", \"a\", \"ver\"]\n",
        "\n",
        "log_likelihood_perms = [(perm, log_likelihood(best_model, perm, processor)) for perm in permutations(sentence)]\n",
        "log_likelihood_perms.sort(key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZoj4Qj_HUzq"
      },
      "source": [
        "Permutaciones con mayor log-verosimilitud:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPrGyVjmHXt3",
        "outputId": "3ba32571-c213-4f77-e163-a4969840d54b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "van ver a hijos de la chingada\n",
            "Log-verosimilitud: -12.792529\n",
            "------------------------------\n",
            "la van a ver hijos de chingada\n",
            "Log-verosimilitud: -15.818754\n",
            "------------------------------\n",
            "hijos van a ver de la chingada\n",
            "Log-verosimilitud: -16.934473\n",
            "------------------------------\n",
            "ver van a hijos de la chingada\n",
            "Log-verosimilitud: -17.56024\n",
            "------------------------------\n",
            "chingada hijos de la van a ver\n",
            "Log-verosimilitud: -19.902334\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "for perm, log_lh in log_likelihood_perms[:5]:\n",
        "    print(\" \".join(perm))\n",
        "    print(\"Log-verosimilitud:\", log_lh)\n",
        "    print(\"-\"*30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g4Fi-IPHYaq"
      },
      "source": [
        "Permutaciones con menor log-verosimilitud:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek-xXACbHal-",
        "outputId": "d5a010eb-c718-472d-8361-32a09351b734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "de a chingada van la hijos ver\n",
            "Log-verosimilitud: -117.186806\n",
            "------------------------------\n",
            "de chingada la a hijos van ver\n",
            "Log-verosimilitud: -117.82024\n",
            "------------------------------\n",
            "la de chingada a hijos van ver\n",
            "Log-verosimilitud: -118.17029\n",
            "------------------------------\n",
            "a de chingada la hijos van ver\n",
            "Log-verosimilitud: -118.603195\n",
            "------------------------------\n",
            "de a chingada la hijos van ver\n",
            "Log-verosimilitud: -122.77367\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "for perm, log_lh in log_likelihood_perms[-5:]:\n",
        "    print(\" \".join(perm))\n",
        "    print(\"Log-verosimilitud:\", log_lh)\n",
        "    print(\"-\"*30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BshJMuIbHW0s"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.6) Evaluación del modelo a través de la perplejidad\n",
        "\n",
        "Calcule la perplejidad del modelo sobre los datos de validación. Compárelo con la perplejidad del modelo de lenguaje sin embeddings preentrenados y el probabilista de la tarea anterior.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED83GwZkFjkn"
      },
      "source": [
        "Primero entrenamos el modelo sin embeddings preentrenados para hacer la comparación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap8m5H9Dnkf4"
      },
      "source": [
        "Hiperparámetros del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHoM21XJnkf7"
      },
      "outputs": [],
      "source": [
        "# Model hyperparameters\n",
        "embedding_dim = 100  # Dimension of word embedding\n",
        "hidden_dim = 200  # Dimension for hidden layer\n",
        "dropout = 0.2\n",
        "\n",
        "# Training hyperparameters\n",
        "lr = 2.3e-1\n",
        "num_epochs = 100\n",
        "patience = 20\n",
        "\n",
        "# Scheduler hyperparameters\n",
        "lr_patience = 10\n",
        "lr_factor = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RKTqLgEFipV"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model_scratch = NeuralLM(\n",
        "    window_size = 3,\n",
        "    embedding_dim = embedding_dim,\n",
        "    hidden_dim = hidden_dim,\n",
        "    vocab_size = vocab_len,\n",
        "    dropout = dropout\n",
        ")\n",
        "\n",
        "# Send to GPU\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    model_scratch.cuda()\n",
        "\n",
        "# Loss, Optimizer and Scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_scratch.parameters(), lr = lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer, \"min\",\n",
        "                patience = lr_patience,\n",
        "                factor = lr_factor\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtzEotPvFipX"
      },
      "source": [
        "Entrenamiento del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1MyHcviFipX",
        "outputId": "5f07f9d4-a85b-4fb1-df9c-dd7b7da02fda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train acc: 0.08506218038121803\n",
            "Epoch [1/100], Loss: 6.4539 - Val accuracy: 0.0942 - Epoch time: 11.17s\n",
            "---11.173033714294434 seconds ---\n",
            "Train acc: 0.1019039109716411\n",
            "Epoch [2/100], Loss: 5.9382 - Val accuracy: 0.1090 - Epoch time: 7.91s\n",
            "---19.082343339920044 seconds ---\n",
            "Train acc: 0.1070831880520688\n",
            "Epoch [3/100], Loss: 5.7054 - Val accuracy: 0.1199 - Epoch time: 7.33s\n",
            "---26.412657260894775 seconds ---\n",
            "Train acc: 0.1120045908879591\n",
            "Epoch [4/100], Loss: 5.5124 - Val accuracy: 0.0992 - Epoch time: 6.21s\n",
            "---32.61926031112671 seconds ---\n",
            "Train acc: 0.11610515457926546\n",
            "Epoch [5/100], Loss: 5.3314 - Val accuracy: 0.1071 - Epoch time: 7.26s\n",
            "---39.881489515304565 seconds ---\n",
            "Train acc: 0.1181136680613668\n",
            "Epoch [6/100], Loss: 5.1573 - Val accuracy: 0.1134 - Epoch time: 6.39s\n",
            "---46.26910924911499 seconds ---\n",
            "Train acc: 0.12061976987447699\n",
            "Epoch [7/100], Loss: 4.9837 - Val accuracy: 0.1234 - Epoch time: 7.34s\n",
            "---53.60463190078735 seconds ---\n",
            "Train acc: 0.1228026208740121\n",
            "Epoch [8/100], Loss: 4.8143 - Val accuracy: 0.1091 - Epoch time: 6.30s\n",
            "---59.89990735054016 seconds ---\n",
            "Train acc: 0.12562107740585773\n",
            "Epoch [9/100], Loss: 4.6466 - Val accuracy: 0.1044 - Epoch time: 7.33s\n",
            "---67.2313346862793 seconds ---\n",
            "Train acc: 0.1320098210134821\n",
            "Epoch [10/100], Loss: 4.4814 - Val accuracy: 0.0845 - Epoch time: 6.42s\n",
            "---73.6536750793457 seconds ---\n",
            "Train acc: 0.14053419920966992\n",
            "Epoch [11/100], Loss: 4.3286 - Val accuracy: 0.0956 - Epoch time: 7.50s\n",
            "---81.15109944343567 seconds ---\n",
            "Train acc: 0.15513859832635984\n",
            "Epoch [12/100], Loss: 4.1752 - Val accuracy: 0.0775 - Epoch time: 7.86s\n",
            "---89.00742506980896 seconds ---\n",
            "Train acc: 0.17608089260808926\n",
            "Epoch [13/100], Loss: 4.0447 - Val accuracy: 0.1018 - Epoch time: 9.60s\n",
            "---98.605952501297 seconds ---\n",
            "Train acc: 0.1944952928870293\n",
            "Epoch [14/100], Loss: 3.9282 - Val accuracy: 0.1008 - Epoch time: 7.36s\n",
            "---105.96982884407043 seconds ---\n",
            "Train acc: 0.2103491108786611\n",
            "Epoch [15/100], Loss: 3.8306 - Val accuracy: 0.1086 - Epoch time: 6.48s\n",
            "---112.45001149177551 seconds ---\n",
            "Train acc: 0.22631552185030218\n",
            "Epoch [16/100], Loss: 3.7435 - Val accuracy: 0.1142 - Epoch time: 7.28s\n",
            "---119.72651743888855 seconds ---\n",
            "Train acc: 0.24146109367735935\n",
            "Epoch [17/100], Loss: 3.6588 - Val accuracy: 0.1100 - Epoch time: 6.36s\n",
            "---126.08886551856995 seconds ---\n",
            "Train acc: 0.25224459553695955\n",
            "Epoch [18/100], Loss: 3.5987 - Val accuracy: 0.1178 - Epoch time: 7.30s\n",
            "---133.39140582084656 seconds ---\n",
            "Train acc: 0.26142637145513714\n",
            "Epoch [19/100], Loss: 3.5334 - Val accuracy: 0.1076 - Epoch time: 6.44s\n",
            "---139.83565855026245 seconds ---\n",
            "Train acc: 0.2680112447698745\n",
            "Epoch [20/100], Loss: 3.4855 - Val accuracy: 0.1004 - Epoch time: 7.18s\n",
            "---147.0192165374756 seconds ---\n",
            "Train acc: 0.27688793003254303\n",
            "Epoch [21/100], Loss: 3.4309 - Val accuracy: 0.0941 - Epoch time: 6.37s\n",
            "---153.38805270195007 seconds ---\n",
            "Train acc: 0.2834401150627615\n",
            "Epoch [22/100], Loss: 3.3879 - Val accuracy: 0.1143 - Epoch time: 8.40s\n",
            "---161.7871196269989 seconds ---\n",
            "Train acc: 0.28901165155741515\n",
            "Epoch [23/100], Loss: 3.3474 - Val accuracy: 0.1025 - Epoch time: 8.69s\n",
            "---170.4798300266266 seconds ---\n",
            "Train acc: 0.31508237447698745\n",
            "Epoch [24/100], Loss: 3.1295 - Val accuracy: 0.1303 - Epoch time: 7.18s\n",
            "---177.6562569141388 seconds ---\n",
            "Train acc: 0.32255709553695955\n",
            "Epoch [25/100], Loss: 3.0851 - Val accuracy: 0.1050 - Epoch time: 6.94s\n",
            "---184.59238266944885 seconds ---\n",
            "Train acc: 0.32324718154346815\n",
            "Epoch [26/100], Loss: 3.0676 - Val accuracy: 0.1223 - Epoch time: 6.95s\n",
            "---191.53809332847595 seconds ---\n",
            "Train acc: 0.32714798349604834\n",
            "Epoch [27/100], Loss: 3.0422 - Val accuracy: 0.1107 - Epoch time: 7.08s\n",
            "---198.61670589447021 seconds ---\n",
            "Train acc: 0.33069647838214783\n",
            "Epoch [28/100], Loss: 3.0186 - Val accuracy: 0.1222 - Epoch time: 7.08s\n",
            "---205.69573402404785 seconds ---\n",
            "Train acc: 0.33377644119014416\n",
            "Epoch [29/100], Loss: 3.0004 - Val accuracy: 0.1220 - Epoch time: 7.28s\n",
            "---212.97254753112793 seconds ---\n",
            "Train acc: 0.3353963272896327\n",
            "Epoch [30/100], Loss: 2.9920 - Val accuracy: 0.1084 - Epoch time: 6.35s\n",
            "---219.32305097579956 seconds ---\n",
            "Train acc: 0.34057923640167365\n",
            "Epoch [31/100], Loss: 2.9714 - Val accuracy: 0.1096 - Epoch time: 7.26s\n",
            "---226.57819247245789 seconds ---\n",
            "Train acc: 0.33972207694095774\n",
            "Epoch [32/100], Loss: 2.9506 - Val accuracy: 0.1181 - Epoch time: 6.37s\n",
            "---232.95257949829102 seconds ---\n",
            "Train acc: 0.3421119537424454\n",
            "Epoch [33/100], Loss: 2.9459 - Val accuracy: 0.1023 - Epoch time: 10.51s\n",
            "---243.4655110836029 seconds ---\n",
            "Train acc: 0.34405509065550904\n",
            "Epoch [34/100], Loss: 2.9263 - Val accuracy: 0.1128 - Epoch time: 8.09s\n",
            "---251.55975914001465 seconds ---\n",
            "Train acc: 0.3630288238028824\n",
            "Epoch [35/100], Loss: 2.7944 - Val accuracy: 0.1279 - Epoch time: 7.33s\n",
            "---258.88801407814026 seconds ---\n",
            "Train acc: 0.36743084611808463\n",
            "Epoch [36/100], Loss: 2.7641 - Val accuracy: 0.1084 - Epoch time: 8.74s\n",
            "---267.62985014915466 seconds ---\n",
            "Train acc: 0.3689272431427243\n",
            "Epoch [37/100], Loss: 2.7581 - Val accuracy: 0.1257 - Epoch time: 8.19s\n",
            "---275.81693601608276 seconds ---\n",
            "Train acc: 0.3702311424918643\n",
            "Epoch [38/100], Loss: 2.7461 - Val accuracy: 0.1225 - Epoch time: 8.90s\n",
            "---284.7194447517395 seconds ---\n",
            "Train acc: 0.37015850185960014\n",
            "Epoch [39/100], Loss: 2.7394 - Val accuracy: 0.1205 - Epoch time: 8.24s\n",
            "---292.9613230228424 seconds ---\n",
            "Train acc: 0.37267549976754993\n",
            "Epoch [40/100], Loss: 2.7271 - Val accuracy: 0.1224 - Epoch time: 6.63s\n",
            "---299.59434819221497 seconds ---\n",
            "Train acc: 0.3715749941887494\n",
            "Epoch [41/100], Loss: 2.7190 - Val accuracy: 0.1168 - Epoch time: 7.88s\n",
            "---307.4695339202881 seconds ---\n",
            "Train acc: 0.37315492794049276\n",
            "Epoch [42/100], Loss: 2.7101 - Val accuracy: 0.1142 - Epoch time: 6.29s\n",
            "---313.7588813304901 seconds ---\n",
            "Train acc: 0.37723369944211993\n",
            "Epoch [43/100], Loss: 2.7031 - Val accuracy: 0.1271 - Epoch time: 7.34s\n",
            "---321.0952818393707 seconds ---\n",
            "No improvement. Breaking out of loop\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "best_metric = 0\n",
        "metric_history = []\n",
        "train_metric_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    loss_epoch = []\n",
        "    training_metric = []\n",
        "    model_scratch.train()\n",
        "\n",
        "    for window_words, labels in train_loader:\n",
        "\n",
        "        # If GPU available\n",
        "        if use_gpu:\n",
        "            window_words = window_words.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_scratch(window_words)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_epoch.append(loss.item())\n",
        "\n",
        "        # Get training metrics\n",
        "        y_pred = get_preds(outputs)\n",
        "        tgt = labels.cpu().numpy()\n",
        "        training_metric.append(accuracy_score(tgt, y_pred))\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Get metric in validation dataset\n",
        "    mean_epoch_metric = np.mean(training_metric)\n",
        "    train_metric_history.append(mean_epoch_metric)\n",
        "\n",
        "    # Get metric in validation dataset\n",
        "    model_scratch.eval()\n",
        "    tuning_metric = model_eval(val_loader, model_scratch, gpu = use_gpu)\n",
        "    metric_history.append(mean_epoch_metric)\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step(tuning_metric)\n",
        "\n",
        "    # Check for metric improvement\n",
        "    is_improvement = tuning_metric > best_metric\n",
        "    if is_improvement:\n",
        "        best_metric = tuning_metric\n",
        "        n_no_improve = 0\n",
        "    else:\n",
        "        n_no_improve += 1\n",
        "\n",
        "    # Save best model if metric improved\n",
        "    save_checkpoint(\n",
        "      {\n",
        "        'epoch': epoch + 1,\n",
        "        'state_dict': model_scratch.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict(),\n",
        "        'best_metric': best_metric,\n",
        "      },\n",
        "      is_improvement,\n",
        "      savedir_scratch\n",
        "    )\n",
        "\n",
        "    if n_no_improve >= patience:\n",
        "        print(\"No improvement. Breaking out of loop\")\n",
        "        break\n",
        "\n",
        "    print('Train acc: {}'.format(mean_epoch_metric))\n",
        "    print('Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}s'.format(\n",
        "        epoch + 1,\n",
        "        num_epochs,\n",
        "        np.mean(loss_epoch),\n",
        "        tuning_metric,\n",
        "        time.time() - epoch_start_time\n",
        "    ))\n",
        "\n",
        "    print(\"---%s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4hzVldxGw5V"
      },
      "source": [
        "Cargamos el mejor modelo obtenido durante el entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ESEe24AGw5X",
        "outputId": "2693a487-9a40-4050-de56-f6c83d03cce5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NeuralLM(\n",
              "  (emb): Embedding(11428, 100)\n",
              "  (dense_1): Linear(in_features=300, out_features=200, bias=True)\n",
              "  (drop1): Dropout(p=0.2, inplace=False)\n",
              "  (dense_2): Linear(in_features=200, out_features=11428, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model_scratch = NeuralLM(\n",
        "    window_size = 3,\n",
        "    embedding_dim = embedding_dim,\n",
        "    hidden_dim = hidden_dim,\n",
        "    vocab_size = vocab_len,\n",
        "    dropout = dropout\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "best_model_scratch.load_state_dict(torch.load(savedir_scratch / \"model_best.pt\", map_location=device)[\"state_dict\"])\n",
        "best_model_scratch.train(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIt1lSK3HI_9"
      },
      "source": [
        "Para calcular la perplejidad de un modelo, podemos utilizar nuestra función que calcula la log-verosimilitud de un texto $X=(x_1, ..., x_N)$, pues justo su perplejidad $PPL(X)$ es\n",
        "$$\n",
        "PPL(X) = \\exp\\left(-\\frac{1}{N} \\log \\mathbb{P}(X) \\right),\n",
        "$$\n",
        "donde $\\log \\mathbb{P}(X)$ es la log-verosimilitud del texto $X$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "edILxkQjoh59"
      },
      "outputs": [],
      "source": [
        "def perplexity(model: NeuralLM, corpus: list[list[str]], text_processor: TextProcessor) -> float:\n",
        "    # corpus is masked inside log_likelihood\n",
        "\n",
        "    corpus_len = sum(len(text) for text in corpus)\n",
        "    log_lh = sum(log_likelihood(model, text, text_processor) for text in corpus)\n",
        "\n",
        "    return math.exp(-log_lh/corpus_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COH_NUp4LO12"
      },
      "source": [
        "Perplejidad del mejor modelo con embeddings preentrenados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlFAaKn8D13X",
        "outputId": "e0053935-6cab-4ba5-8abc-0985feeb9718"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "602.0006596709721"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perplexity(best_model, val_corpus_tk, processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZAS3zT1Lcqw"
      },
      "source": [
        "Perplejidad del mejor modelo entrenado desde cero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyzmzeKsEtec",
        "outputId": "f605bb72-f084-4124-86fd-ec6e1db1bbce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "185.10026931719662"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perplexity(best_model_scratch, val_corpus_tk, processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpTjKsUsHfiw"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.7) Discusión\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9ahv-EXnbvn"
      },
      "source": [
        "A través el ejercicio de encontrar palabras similares, notamos que el modelo de lenguaje neuronal sí logra aprender algo del texto. Por ejemplo, el modelo encuentra similar la palabra \"madre\" a las palabras \"abuela\", \"mama\", \"papá\", etc. O la palabra \"chingada\" la encuentra similar a otras como \"fregada\" y \"verga\", lo cual tiene sentido pues esas palabras se suelen usar en frases similares como \"veta a la ...\"). También si fijamos una oración y permutamos sus tokens, se observa que las permutaciones con mayor verosimilitud sí tienen algo de estructura sintáctica.\n",
        "\n",
        "Algo curioso es que el modelo entrenado desde cero tuvo una menor perplejidad (185.1) que el modelo con los embeddings preentrenados (602.0). Lo que yo sospecho es que al entrenar desde cero el modelo, se logra encontrar embeddings que funcionan para este corpus en particular, obteniendo así menor perplejidad. Mientras que con los embeddings preentrenados, seguramente el corpus con el que se entrenaron era de caracter general, y entonces al proceso de entrenamiento le cuesta más adaptar los embeddings a este contexto en particular.\n",
        "\n",
        "La perplejidad del mejor modelo de lenguaje (interpolado) basado en frecuencias de la tarea anterior fue de 138.74. Este valor es cercano al obtenido con el modelo de lenguaje neuronal entrenado desde 0. Sin embargo, hay que tomar en cuenta que los corpus de esta tarea y los de la tarea anterior son diferentes, por lo que la perplejidad puede que no sea comparable. El corpus de la tarea anterior, con las conferencias de prensa de los presidentes de México, tiene un lenguaje más formal, por lo que pudiera tener más estructura, y por lo tanto se podría esperar que un modelo tuviera una perplejidad menor sobre ese corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qKEJmdPJj7M"
      },
      "source": [
        "# 2) Modelo de Lenguaje Neuronal (Bengio 2003) a nivel de caracter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzjBFUAYv3y3"
      },
      "source": [
        "---\n",
        "\n",
        "## 2.1) Implementación del modelo\n",
        "\n",
        "Con base en la implementación mostrada en las prácticas del NLM, construya un modelo de lenguaje neuronal a nivel de caracter. Tome en cuenta secuencias de tamaño 6 o más para el modelo, es decir hasta 5 caracteres o más en el contexto.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EgkluKvQSe4"
      },
      "source": [
        "Notemos que si hacemos ``list(text)``, donde ``text`` es una string, vamos a obtener la tokenización por caracter. Como vocabulario vamos a utilizar todos los caracteres encontrados en el conjunto de entrenamiento pero sin considerar signos de puntuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "R6zqTtXpHO7E"
      },
      "outputs": [],
      "source": [
        "train_tk_char = [list(text) for text in train_corpus]\n",
        "val_tk_char = [list(text) for text in val_corpus]\n",
        "\n",
        "vocab_char = set()\n",
        "for text in train_tk_char:\n",
        "    vocab_char.update(text)\n",
        "\n",
        "vocab_char.update([INIT_TKN, END_TKN, UNK_TKN])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZK9qEYWtmUU",
        "outputId": "ef36e244-d8ca-4752-c9cb-bc52852ae354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del vocabulario: 429\n"
          ]
        }
      ],
      "source": [
        "vocab_char_len = len(vocab_char)\n",
        "print(\"Tamaño del vocabulario:\", vocab_char_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRKKHnlmRLLi"
      },
      "source": [
        "Cargamos el mapeo ``{ char -> char id }`` si es que ya se había calculado antes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "F41OWUbhQ5mx"
      },
      "outputs": [],
      "source": [
        "char2id_path = base_path / \"models-tarea-5\" / \"char2id.json\"\n",
        "\n",
        "if char2id_path.exists():\n",
        "    with open(char2id_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        char2id_json = json.load(file)\n",
        "else:\n",
        "    char2id_json = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5o_TSk9DQ5mz"
      },
      "outputs": [],
      "source": [
        "processor_char = TextProcessor(vocab=vocab_char, word2id=char2id_json)\n",
        "train_char_msk = processor_char.transform(train_tk_char)\n",
        "val_char_msk = processor_char.transform(val_tk_char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw7UGi8PQ5m0"
      },
      "source": [
        "Si no habíamos creado el archivo json antes, lo hacemos con el mapeo calculado en ``processor_char``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "uGcGB8HRQ5m0"
      },
      "outputs": [],
      "source": [
        "if char2id_json is None:\n",
        "    with open(char2id_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        json.dump(processor_char.word2id, file, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioSRg2CAofpu"
      },
      "source": [
        "Porcentaje de tokens ``<UNK>`` en el corpus de validación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5UC5fUQoq36",
        "outputId": "5bec1c4a-3787-4719-cfcf-99cadcaae11d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.03829290241053821"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(\n",
        "    sum([1 if tkn == UNK_TKN else 0 for tkn in tweet]) for tweet in val_char_msk\n",
        ") / sum(len(tweet) for tweet in val_char_msk) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfEP0kttpYBi"
      },
      "source": [
        "Vemos que hay algunos caracteres que no estamos considerando en el conjunto de entrenamiento. Pero dado que utilizamos como vocabulario todos los caracteres del conjunto de entrenamiento, entonces no se entrenaría un embedding para el token ``<UNK>``. De tal manera que en el conjunto de validación vamos a quitarlos. Además en la siguiente celda se pueden observar que justo esos caracteres fuera del vocabulario son en su mayoría emojis, por lo que no afectaría tanto quitarlos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTcrm0V_pEtD",
        "outputId": "eb7dfb37-5cb8-4c26-adb7-5910dd439858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💥\n",
            "🆙\n",
            "👱\n",
            "📧\n",
            "🏈\n",
            "♠\n",
            "🐃\n",
            "🖤\n",
            "🍠\n",
            "💰\n",
            "̶\n",
            "̶\n",
            "̶\n",
            "̶\n",
            "̶\n",
            "̶\n",
            "🍾\n",
            "🤚\n",
            "😼\n",
            "😼\n"
          ]
        }
      ],
      "source": [
        "for tweet in val_tk_char:\n",
        "    for tkn in tweet:\n",
        "        if tkn not in processor_char.vocab:\n",
        "            print(tkn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7eB0DCeqgO3"
      },
      "source": [
        "Filtración de caracteres desconocidos en corpus de validación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "BBxz1fAkqVWx"
      },
      "outputs": [],
      "source": [
        "val_tk_char = [[tkn for tkn in tweet if tkn in processor_char.vocab] for tweet in val_tk_char]\n",
        "val_char_msk = processor_char.transform(val_tk_char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGH9G0MGqsiH"
      },
      "source": [
        "Generamos conjunto de entrenamiento y validación con los n-gramas de los corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "S8OQWwqCqx6Y"
      },
      "outputs": [],
      "source": [
        "X_char_train, y_char_train = get_ngrams(masked_corpus=train_char_msk, n=4, text_processor=processor_char)\n",
        "X_char_val, y_char_val = get_ngrams(masked_corpus=val_char_msk, n=4, text_processor=processor_char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmuXfjlLtBBU"
      },
      "source": [
        "Definimos los objetos ``TensorDataset`` y ``DataLoader`` con nuestros datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "undH9SKitBqM"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# training\n",
        "train_char_dataset = TensorDataset(\n",
        "    torch.tensor(X_char_train, dtype=torch.int64), torch.tensor(y_char_train, dtype=torch.int64)\n",
        ")\n",
        "\n",
        "train_char_loader = DataLoader(\n",
        "    train_char_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True\n",
        ")\n",
        "\n",
        "# validation\n",
        "val_char_dataset = TensorDataset(\n",
        "    torch.tensor(X_char_val, dtype=torch.int64), torch.tensor(y_char_val, dtype=torch.int64)\n",
        ")\n",
        "\n",
        "val_char_loader = DataLoader(\n",
        "    val_char_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpVFBe37tZC3"
      },
      "source": [
        "Hiperparámetros del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "e0YjgScitbJg"
      },
      "outputs": [],
      "source": [
        "# Model hyperparameters\n",
        "embedding_dim = 100  # Dimension of word embedding\n",
        "hidden_dim = 200  # Dimension for hidden layer\n",
        "dropout = 0.2\n",
        "\n",
        "# Training hyperparameters\n",
        "lr = 2.3e-1\n",
        "num_epochs = 100\n",
        "patience = 20\n",
        "\n",
        "# Scheduler hyperparameters\n",
        "lr_patience = 10\n",
        "lr_factor = 0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9jhCFActeCK"
      },
      "source": [
        "Modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "xjsWy5AQtfhJ"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model_char = NeuralLM(\n",
        "    window_size = 3,\n",
        "    embedding_dim = embedding_dim,\n",
        "    hidden_dim = hidden_dim,\n",
        "    vocab_size = vocab_char_len,\n",
        "    dropout = dropout\n",
        ")\n",
        "\n",
        "# Send to GPU\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    model_char.cuda()\n",
        "\n",
        "# Loss, Optimizer and Scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_char.parameters(), lr = lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer, \"min\",\n",
        "                patience = lr_patience,\n",
        "                factor = lr_factor\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB_FRPeFt8Jf"
      },
      "source": [
        "Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGXLyb9ft6i5",
        "outputId": "2d179ef1-3b51-4d5b-e86c-f5e42c14ed40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train acc: 0.39974606701450194\n",
            "Epoch [1/100], Loss: 2.1194 - Val accuracy: 0.4389 - Epoch time: 42.66s\n",
            "---42.663923501968384 seconds ---\n",
            "Train acc: 0.42846156394094925\n",
            "Epoch [2/100], Loss: 1.9779 - Val accuracy: 0.4478 - Epoch time: 28.78s\n",
            "---71.44033074378967 seconds ---\n",
            "Train acc: 0.4364697700007652\n",
            "Epoch [3/100], Loss: 1.9363 - Val accuracy: 0.4503 - Epoch time: 29.11s\n",
            "---100.5498297214508 seconds ---\n",
            "Train acc: 0.44156003169911207\n",
            "Epoch [4/100], Loss: 1.9107 - Val accuracy: 0.4554 - Epoch time: 32.10s\n",
            "---132.65417861938477 seconds ---\n",
            "Train acc: 0.4449938660021709\n",
            "Epoch [5/100], Loss: 1.8944 - Val accuracy: 0.4605 - Epoch time: 29.49s\n",
            "---162.14362502098083 seconds ---\n",
            "Train acc: 0.4469325494049863\n",
            "Epoch [6/100], Loss: 1.8811 - Val accuracy: 0.4634 - Epoch time: 38.41s\n",
            "---200.55559015274048 seconds ---\n",
            "Train acc: 0.44944836530640586\n",
            "Epoch [7/100], Loss: 1.8706 - Val accuracy: 0.4640 - Epoch time: 42.30s\n",
            "---242.85403394699097 seconds ---\n",
            "Train acc: 0.4513575123494753\n",
            "Epoch [8/100], Loss: 1.8626 - Val accuracy: 0.4653 - Epoch time: 29.48s\n",
            "---272.330926656723 seconds ---\n",
            "Train acc: 0.45209158166807895\n",
            "Epoch [9/100], Loss: 1.8549 - Val accuracy: 0.4640 - Epoch time: 30.15s\n",
            "---302.47701263427734 seconds ---\n",
            "Train acc: 0.4543922736779756\n",
            "Epoch [10/100], Loss: 1.8486 - Val accuracy: 0.4655 - Epoch time: 46.66s\n",
            "---349.1361291408539 seconds ---\n",
            "Train acc: 0.45530860934847484\n",
            "Epoch [11/100], Loss: 1.8448 - Val accuracy: 0.4654 - Epoch time: 36.01s\n",
            "---385.1483681201935 seconds ---\n",
            "Train acc: 0.4553875648648572\n",
            "Epoch [12/100], Loss: 1.8392 - Val accuracy: 0.4666 - Epoch time: 34.68s\n",
            "---419.83319544792175 seconds ---\n",
            "Train acc: 0.46228397274754923\n",
            "Epoch [13/100], Loss: 1.8154 - Val accuracy: 0.4743 - Epoch time: 40.73s\n",
            "---460.55928087234497 seconds ---\n",
            "Train acc: 0.4622739649255064\n",
            "Epoch [14/100], Loss: 1.8107 - Val accuracy: 0.4746 - Epoch time: 36.25s\n",
            "---496.8141658306122 seconds ---\n",
            "Train acc: 0.4636132417833124\n",
            "Epoch [15/100], Loss: 1.8084 - Val accuracy: 0.4740 - Epoch time: 46.34s\n",
            "---543.1552813053131 seconds ---\n",
            "Train acc: 0.46381521380250473\n",
            "Epoch [16/100], Loss: 1.8054 - Val accuracy: 0.4727 - Epoch time: 32.57s\n",
            "---575.7294476032257 seconds ---\n",
            "Train acc: 0.4638859770751795\n",
            "Epoch [17/100], Loss: 1.8040 - Val accuracy: 0.4751 - Epoch time: 37.62s\n",
            "---613.3530180454254 seconds ---\n",
            "Train acc: 0.46393105655677075\n",
            "Epoch [18/100], Loss: 1.8033 - Val accuracy: 0.4742 - Epoch time: 37.16s\n",
            "---650.5096592903137 seconds ---\n",
            "Train acc: 0.463651811752336\n",
            "Epoch [19/100], Loss: 1.8019 - Val accuracy: 0.4728 - Epoch time: 35.43s\n",
            "---685.938282251358 seconds ---\n",
            "Train acc: 0.4649027895076931\n",
            "Epoch [20/100], Loss: 1.8003 - Val accuracy: 0.4757 - Epoch time: 40.63s\n",
            "---726.5663537979126 seconds ---\n",
            "Train acc: 0.464739874563907\n",
            "Epoch [21/100], Loss: 1.7998 - Val accuracy: 0.4722 - Epoch time: 29.30s\n",
            "---755.8707909584045 seconds ---\n",
            "Train acc: 0.4651416487647691\n",
            "Epoch [22/100], Loss: 1.7971 - Val accuracy: 0.4754 - Epoch time: 29.25s\n",
            "---785.1226906776428 seconds ---\n",
            "Train acc: 0.46534942177815436\n",
            "Epoch [23/100], Loss: 1.7959 - Val accuracy: 0.4737 - Epoch time: 29.87s\n",
            "---814.9963743686676 seconds ---\n",
            "Train acc: 0.46887226370203683\n",
            "Epoch [24/100], Loss: 1.7819 - Val accuracy: 0.4788 - Epoch time: 35.69s\n",
            "---850.6831233501434 seconds ---\n",
            "Train acc: 0.46944975045982845\n",
            "Epoch [25/100], Loss: 1.7797 - Val accuracy: 0.4783 - Epoch time: 29.61s\n",
            "---880.2964985370636 seconds ---\n",
            "Train acc: 0.4695036864211036\n",
            "Epoch [26/100], Loss: 1.7791 - Val accuracy: 0.4786 - Epoch time: 30.85s\n",
            "---911.147791147232 seconds ---\n",
            "Train acc: 0.4696951192294721\n",
            "Epoch [27/100], Loss: 1.7783 - Val accuracy: 0.4766 - Epoch time: 29.44s\n",
            "---940.5918090343475 seconds ---\n",
            "Train acc: 0.46943792705945037\n",
            "Epoch [28/100], Loss: 1.7780 - Val accuracy: 0.4760 - Epoch time: 29.45s\n",
            "---970.0437822341919 seconds ---\n",
            "Train acc: 0.47028234811491604\n",
            "Epoch [29/100], Loss: 1.7769 - Val accuracy: 0.4776 - Epoch time: 29.31s\n",
            "---999.3505878448486 seconds ---\n",
            "Train acc: 0.470210389217484\n",
            "Epoch [30/100], Loss: 1.7757 - Val accuracy: 0.4764 - Epoch time: 30.65s\n",
            "---1030.002980709076 seconds ---\n",
            "Train acc: 0.4703130358170209\n",
            "Epoch [31/100], Loss: 1.7750 - Val accuracy: 0.4779 - Epoch time: 30.56s\n",
            "---1060.5621733665466 seconds ---\n",
            "Train acc: 0.4699995607186077\n",
            "Epoch [32/100], Loss: 1.7753 - Val accuracy: 0.4760 - Epoch time: 29.22s\n",
            "---1089.7843835353851 seconds ---\n",
            "Train acc: 0.4707376597354676\n",
            "Epoch [33/100], Loss: 1.7744 - Val accuracy: 0.4785 - Epoch time: 30.90s\n",
            "---1120.6882593631744 seconds ---\n",
            "Train acc: 0.4706296549657219\n",
            "Epoch [34/100], Loss: 1.7738 - Val accuracy: 0.4766 - Epoch time: 29.25s\n",
            "---1149.9413814544678 seconds ---\n",
            "Train acc: 0.47282854174165156\n",
            "Epoch [35/100], Loss: 1.7653 - Val accuracy: 0.4800 - Epoch time: 30.93s\n",
            "---1180.8674914836884 seconds ---\n",
            "Train acc: 0.4735427725457632\n",
            "Epoch [36/100], Loss: 1.7635 - Val accuracy: 0.4805 - Epoch time: 31.30s\n",
            "---1212.168942451477 seconds ---\n",
            "Train acc: 0.4734678024452386\n",
            "Epoch [37/100], Loss: 1.7620 - Val accuracy: 0.4790 - Epoch time: 29.31s\n",
            "---1241.480548620224 seconds ---\n",
            "Train acc: 0.47290382181896506\n",
            "Epoch [38/100], Loss: 1.7618 - Val accuracy: 0.4825 - Epoch time: 29.06s\n",
            "---1270.5364117622375 seconds ---\n",
            "Train acc: 0.473561105458709\n",
            "Epoch [39/100], Loss: 1.7612 - Val accuracy: 0.4802 - Epoch time: 30.32s\n",
            "---1300.8520650863647 seconds ---\n",
            "Train acc: 0.4739014599729629\n",
            "Epoch [40/100], Loss: 1.7606 - Val accuracy: 0.4789 - Epoch time: 30.95s\n",
            "---1331.7983100414276 seconds ---\n",
            "Train acc: 0.47372619024001766\n",
            "Epoch [41/100], Loss: 1.7607 - Val accuracy: 0.4805 - Epoch time: 30.82s\n",
            "---1362.6177816390991 seconds ---\n",
            "Train acc: 0.47357297314148544\n",
            "Epoch [42/100], Loss: 1.7606 - Val accuracy: 0.4828 - Epoch time: 34.64s\n",
            "---1397.2544829845428 seconds ---\n",
            "Train acc: 0.47402279374463296\n",
            "Epoch [43/100], Loss: 1.7607 - Val accuracy: 0.4795 - Epoch time: 38.45s\n",
            "---1435.707516670227 seconds ---\n",
            "Train acc: 0.4742294154156594\n",
            "Epoch [44/100], Loss: 1.7599 - Val accuracy: 0.4788 - Epoch time: 41.03s\n",
            "---1476.7423779964447 seconds ---\n",
            "Train acc: 0.47339336373349505\n",
            "Epoch [45/100], Loss: 1.7593 - Val accuracy: 0.4789 - Epoch time: 40.76s\n",
            "---1517.502699136734 seconds ---\n",
            "Train acc: 0.4752582726605432\n",
            "Epoch [46/100], Loss: 1.7528 - Val accuracy: 0.4815 - Epoch time: 37.65s\n",
            "---1555.1565036773682 seconds ---\n",
            "Train acc: 0.4760015084356198\n",
            "Epoch [47/100], Loss: 1.7517 - Val accuracy: 0.4812 - Epoch time: 30.76s\n",
            "---1585.920705318451 seconds ---\n",
            "Train acc: 0.4755518649620659\n",
            "Epoch [48/100], Loss: 1.7513 - Val accuracy: 0.4814 - Epoch time: 35.85s\n",
            "---1621.766057252884 seconds ---\n",
            "Train acc: 0.4761614121763134\n",
            "Epoch [49/100], Loss: 1.7506 - Val accuracy: 0.4823 - Epoch time: 34.41s\n",
            "---1656.1785204410553 seconds ---\n",
            "Train acc: 0.4760960070738474\n",
            "Epoch [50/100], Loss: 1.7509 - Val accuracy: 0.4824 - Epoch time: 35.87s\n",
            "---1692.0453221797943 seconds ---\n",
            "Train acc: 0.4763063041839427\n",
            "Epoch [51/100], Loss: 1.7508 - Val accuracy: 0.4806 - Epoch time: 37.08s\n",
            "---1729.1230692863464 seconds ---\n",
            "Train acc: 0.47581875497734155\n",
            "Epoch [52/100], Loss: 1.7510 - Val accuracy: 0.4823 - Epoch time: 34.79s\n",
            "---1763.9140090942383 seconds ---\n",
            "Train acc: 0.47595859879155106\n",
            "Epoch [53/100], Loss: 1.7504 - Val accuracy: 0.4819 - Epoch time: 37.37s\n",
            "---1801.2792944908142 seconds ---\n",
            "Train acc: 0.4760878148301398\n",
            "Epoch [54/100], Loss: 1.7497 - Val accuracy: 0.4818 - Epoch time: 30.11s\n",
            "---1831.3843803405762 seconds ---\n",
            "Train acc: 0.4753759309931444\n",
            "Epoch [55/100], Loss: 1.7496 - Val accuracy: 0.4828 - Epoch time: 30.71s\n",
            "---1862.0938818454742 seconds ---\n",
            "Train acc: 0.4760175386638477\n",
            "Epoch [56/100], Loss: 1.7493 - Val accuracy: 0.4828 - Epoch time: 30.20s\n",
            "---1892.2914016246796 seconds ---\n",
            "Train acc: 0.4770676071775745\n",
            "Epoch [57/100], Loss: 1.7451 - Val accuracy: 0.4829 - Epoch time: 30.52s\n",
            "---1922.810670375824 seconds ---\n",
            "Train acc: 0.4775567948329172\n",
            "Epoch [58/100], Loss: 1.7447 - Val accuracy: 0.4815 - Epoch time: 30.89s\n",
            "---1953.6985368728638 seconds ---\n",
            "Train acc: 0.4778009679778035\n",
            "Epoch [59/100], Loss: 1.7437 - Val accuracy: 0.4826 - Epoch time: 33.39s\n",
            "---1987.0852558612823 seconds ---\n",
            "Train acc: 0.47770819635311423\n",
            "Epoch [60/100], Loss: 1.7432 - Val accuracy: 0.4828 - Epoch time: 29.99s\n",
            "---2017.0791311264038 seconds ---\n",
            "Train acc: 0.4781815309098226\n",
            "Epoch [61/100], Loss: 1.7426 - Val accuracy: 0.4827 - Epoch time: 38.18s\n",
            "---2055.2603828907013 seconds ---\n",
            "Train acc: 0.47783169996230684\n",
            "Epoch [62/100], Loss: 1.7435 - Val accuracy: 0.4817 - Epoch time: 40.40s\n",
            "---2095.663709640503 seconds ---\n",
            "Train acc: 0.4777155029488535\n",
            "Epoch [63/100], Loss: 1.7433 - Val accuracy: 0.4837 - Epoch time: 36.86s\n",
            "---2132.519615650177 seconds ---\n",
            "Train acc: 0.4776113950301687\n",
            "Epoch [64/100], Loss: 1.7429 - Val accuracy: 0.4829 - Epoch time: 33.31s\n",
            "---2165.8290870189667 seconds ---\n",
            "Train acc: 0.4780229556410816\n",
            "Epoch [65/100], Loss: 1.7432 - Val accuracy: 0.4818 - Epoch time: 36.11s\n",
            "---2201.9383828639984 seconds ---\n",
            "Train acc: 0.47793217672432114\n",
            "Epoch [66/100], Loss: 1.7437 - Val accuracy: 0.4816 - Epoch time: 38.48s\n",
            "---2240.417063474655 seconds ---\n",
            "Train acc: 0.4775431115718055\n",
            "Epoch [67/100], Loss: 1.7429 - Val accuracy: 0.4830 - Epoch time: 30.12s\n",
            "---2270.5352568626404 seconds ---\n",
            "Train acc: 0.4780827368789482\n",
            "Epoch [68/100], Loss: 1.7399 - Val accuracy: 0.4835 - Epoch time: 30.87s\n",
            "---2301.4058163166046 seconds ---\n",
            "Train acc: 0.4786441934085119\n",
            "Epoch [69/100], Loss: 1.7379 - Val accuracy: 0.4840 - Epoch time: 32.84s\n",
            "---2334.247560977936 seconds ---\n",
            "Train acc: 0.47809141622903845\n",
            "Epoch [70/100], Loss: 1.7391 - Val accuracy: 0.4846 - Epoch time: 30.59s\n",
            "---2364.83487033844 seconds ---\n",
            "Train acc: 0.4785686033544094\n",
            "Epoch [71/100], Loss: 1.7385 - Val accuracy: 0.4817 - Epoch time: 29.90s\n",
            "---2394.7397656440735 seconds ---\n",
            "Train acc: 0.47856444080895794\n",
            "Epoch [72/100], Loss: 1.7378 - Val accuracy: 0.4834 - Epoch time: 30.01s\n",
            "---2424.754669189453 seconds ---\n",
            "Train acc: 0.4789122790485449\n",
            "Epoch [73/100], Loss: 1.7389 - Val accuracy: 0.4838 - Epoch time: 30.72s\n",
            "---2455.4781577587128 seconds ---\n",
            "Train acc: 0.478751711071875\n",
            "Epoch [74/100], Loss: 1.7391 - Val accuracy: 0.4836 - Epoch time: 30.27s\n",
            "---2485.7524399757385 seconds ---\n",
            "Train acc: 0.47906115647203196\n",
            "Epoch [75/100], Loss: 1.7384 - Val accuracy: 0.4830 - Epoch time: 31.36s\n",
            "---2517.113266468048 seconds ---\n",
            "Train acc: 0.4787760663910058\n",
            "Epoch [76/100], Loss: 1.7385 - Val accuracy: 0.4829 - Epoch time: 29.14s\n",
            "---2546.253530025482 seconds ---\n",
            "Train acc: 0.47916628288588037\n",
            "Epoch [77/100], Loss: 1.7376 - Val accuracy: 0.4821 - Epoch time: 29.48s\n",
            "---2575.732166290283 seconds ---\n",
            "Train acc: 0.4786188195942173\n",
            "Epoch [78/100], Loss: 1.7390 - Val accuracy: 0.4828 - Epoch time: 29.60s\n",
            "---2605.328016281128 seconds ---\n",
            "Train acc: 0.47940080246791117\n",
            "Epoch [79/100], Loss: 1.7359 - Val accuracy: 0.4834 - Epoch time: 29.97s\n",
            "---2635.2983210086823 seconds ---\n",
            "Train acc: 0.47993126131858105\n",
            "Epoch [80/100], Loss: 1.7348 - Val accuracy: 0.4832 - Epoch time: 29.48s\n",
            "---2664.779445171356 seconds ---\n",
            "Train acc: 0.4807577922850851\n",
            "Epoch [81/100], Loss: 1.7345 - Val accuracy: 0.4845 - Epoch time: 30.22s\n",
            "---2695.0044190883636 seconds ---\n",
            "Train acc: 0.47943370428993703\n",
            "Epoch [82/100], Loss: 1.7343 - Val accuracy: 0.4846 - Epoch time: 31.55s\n",
            "---2726.5502257347107 seconds ---\n",
            "Train acc: 0.4798416337441795\n",
            "Epoch [83/100], Loss: 1.7354 - Val accuracy: 0.4822 - Epoch time: 29.55s\n",
            "---2756.097184419632 seconds ---\n",
            "Train acc: 0.47921587917211045\n",
            "Epoch [84/100], Loss: 1.7353 - Val accuracy: 0.4836 - Epoch time: 31.43s\n",
            "---2787.5267746448517 seconds ---\n",
            "Train acc: 0.4800475911792297\n",
            "Epoch [85/100], Loss: 1.7353 - Val accuracy: 0.4833 - Epoch time: 31.83s\n",
            "---2819.3577473163605 seconds ---\n",
            "Train acc: 0.47947209712936695\n",
            "Epoch [86/100], Loss: 1.7339 - Val accuracy: 0.4834 - Epoch time: 31.20s\n",
            "---2850.5603535175323 seconds ---\n",
            "Train acc: 0.4796253142278992\n",
            "Epoch [87/100], Loss: 1.7351 - Val accuracy: 0.4837 - Epoch time: 31.85s\n",
            "---2882.4153819084167 seconds ---\n",
            "Train acc: 0.48049608331042454\n",
            "Epoch [88/100], Loss: 1.7336 - Val accuracy: 0.4830 - Epoch time: 30.66s\n",
            "---2913.0724527835846 seconds ---\n",
            "Train acc: 0.480422485964251\n",
            "Epoch [89/100], Loss: 1.7331 - Val accuracy: 0.4834 - Epoch time: 30.76s\n",
            "---2943.835818529129 seconds ---\n",
            "Train acc: 0.4802287061887663\n",
            "Epoch [90/100], Loss: 1.7324 - Val accuracy: 0.4837 - Epoch time: 30.50s\n",
            "---2974.331379175186 seconds ---\n",
            "Train acc: 0.48022051394505866\n",
            "Epoch [91/100], Loss: 1.7330 - Val accuracy: 0.4836 - Epoch time: 30.76s\n",
            "---3005.09335398674 seconds ---\n",
            "Train acc: 0.4801602456008094\n",
            "Epoch [92/100], Loss: 1.7322 - Val accuracy: 0.4837 - Epoch time: 30.94s\n",
            "---3036.031842947006 seconds ---\n",
            "Train acc: 0.48055117061405866\n",
            "Epoch [93/100], Loss: 1.7328 - Val accuracy: 0.4833 - Epoch time: 32.51s\n",
            "---3068.539264678955 seconds ---\n",
            "Train acc: 0.4815297673367361\n",
            "Epoch [94/100], Loss: 1.7325 - Val accuracy: 0.4847 - Epoch time: 31.07s\n",
            "---3099.604778289795 seconds ---\n",
            "Train acc: 0.48019314742283525\n",
            "Epoch [95/100], Loss: 1.7322 - Val accuracy: 0.4838 - Epoch time: 29.32s\n",
            "---3128.9287860393524 seconds ---\n",
            "Train acc: 0.4801323919722034\n",
            "Epoch [96/100], Loss: 1.7317 - Val accuracy: 0.4846 - Epoch time: 30.58s\n",
            "---3159.51349401474 seconds ---\n",
            "Train acc: 0.48041097254066184\n",
            "Epoch [97/100], Loss: 1.7321 - Val accuracy: 0.4834 - Epoch time: 30.07s\n",
            "---3189.5824387073517 seconds ---\n",
            "Train acc: 0.4804485240145218\n",
            "Epoch [98/100], Loss: 1.7315 - Val accuracy: 0.4827 - Epoch time: 31.00s\n",
            "---3220.57813000679 seconds ---\n",
            "Train acc: 0.48072440335667666\n",
            "Epoch [99/100], Loss: 1.7318 - Val accuracy: 0.4838 - Epoch time: 30.38s\n",
            "---3250.9569973945618 seconds ---\n",
            "Train acc: 0.4806768440607739\n",
            "Epoch [100/100], Loss: 1.7310 - Val accuracy: 0.4841 - Epoch time: 29.40s\n",
            "---3280.359406709671 seconds ---\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "best_metric = 0\n",
        "metric_history = []\n",
        "train_metric_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    loss_epoch = []\n",
        "    training_metric = []\n",
        "    model_char.train()\n",
        "\n",
        "    for window_words, labels in train_char_loader:\n",
        "\n",
        "        # If GPU available\n",
        "        if use_gpu:\n",
        "            window_words = window_words.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_char(window_words)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_epoch.append(loss.item())\n",
        "\n",
        "        # Get training metrics\n",
        "        y_pred = get_preds(outputs)\n",
        "        tgt = labels.cpu().numpy()\n",
        "        training_metric.append(accuracy_score(tgt, y_pred))\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Get metric in validation dataset\n",
        "    mean_epoch_metric = np.mean(training_metric)\n",
        "    train_metric_history.append(mean_epoch_metric)\n",
        "\n",
        "    # Get metric in validation dataset\n",
        "    model_char.eval()\n",
        "    tuning_metric = model_eval(val_char_loader, model_char, gpu = use_gpu)\n",
        "    metric_history.append(mean_epoch_metric)\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step(tuning_metric)\n",
        "\n",
        "    # Check for metric improvement\n",
        "    is_improvement = tuning_metric > best_metric\n",
        "    if is_improvement:\n",
        "        best_metric = tuning_metric\n",
        "        n_no_improve = 0\n",
        "    else:\n",
        "        n_no_improve += 1\n",
        "\n",
        "    # Save best model if metric improved\n",
        "    save_checkpoint(\n",
        "      {\n",
        "        'epoch': epoch + 1,\n",
        "        'state_dict': model_char.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict(),\n",
        "        'best_metric': best_metric,\n",
        "      },\n",
        "      is_improvement,\n",
        "      savedir_char\n",
        "    )\n",
        "\n",
        "    if n_no_improve >= patience:\n",
        "        print(\"No improvement. Breaking out of loop\")\n",
        "        break\n",
        "\n",
        "    print('Train acc: {}'.format(mean_epoch_metric))\n",
        "    print('Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}s'.format(\n",
        "        epoch + 1,\n",
        "        num_epochs,\n",
        "        np.mean(loss_epoch),\n",
        "        tuning_metric,\n",
        "        time.time() - epoch_start_time\n",
        "    ))\n",
        "\n",
        "    print(\"---%s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OBzLt1kvJGi"
      },
      "source": [
        "Cargamos el mejor modelo obtenido durante el entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihCirM4AvKYC",
        "outputId": "cde99004-a79c-4d33-b7b2-46a9b2259f30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NeuralLM(\n",
              "  (emb): Embedding(429, 100)\n",
              "  (dense_1): Linear(in_features=300, out_features=200, bias=True)\n",
              "  (drop1): Dropout(p=0.2, inplace=False)\n",
              "  (dense_2): Linear(in_features=200, out_features=429, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model_char = NeuralLM(\n",
        "    window_size = 3,\n",
        "    embedding_dim = embedding_dim,\n",
        "    hidden_dim = hidden_dim,\n",
        "    vocab_size = vocab_char_len,\n",
        "    dropout = dropout\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "best_model_char.load_state_dict(torch.load(savedir_char / \"model_best.pt\", map_location=device)[\"state_dict\"])\n",
        "best_model_char.train(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiAoAk0CIo-d"
      },
      "source": [
        "---\n",
        "\n",
        "## 2.2) Generación de texto\n",
        "\n",
        "Ponga al modelo a generar texto 3 veces, con un máximo de 300 caracteres.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TCjT241u-ud"
      },
      "source": [
        "Ejemplos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pevZvg6vvATR",
        "outputId": "a46e71e1-af65-4a55-b392-f87182049068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s><s><s>Me de palar cuando que puta la gorda ahora que se a Madre la verga en femir esta es la ven a está te del amos amos ando seguiero para para de para y Por porque no chinga madre mi mamar pero que pero que estoy vuelven mis no su me esta esta y putas putos verga de no madre muy habe y la y por con loca\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "sequence_1 = [\"<s>\", \"<s>\", \"<s>\"]\n",
        "print(\"\".join(generate_text(best_model_char, processor_char, seed=sequence_1, n_tokens=300, temperature=0.6, top_k=50)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryrd3dh40s18",
        "outputId": "e2a3dd61-1ce2-419e-d652-5b5ec921d941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hola con mueres verga cula ver mi a te las a estoy me duelva a la verga el  son la madre a tien a la la mejo si que en ser del cabron madre si de una haz te para que cribio que putos para por a las madre se la ponse es de vas ten en esta esta y putas putos verga de no madre muy habe y la y por con loca\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "sequence_2 = [\"h\", \"o\", \"l\"]\n",
        "print(\"\".join(generate_text(best_model_char, processor_char, seed=sequence_2, n_tokens=300, temperature=0.6, top_k=50)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fwSyn7x0uNr",
        "outputId": "9c9578c0-b918-4730-f9c5-cf32c95d932c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hijos lleva a con que sigues que está todo me de para de putos gusta fiera es que estar compezó no de te se va a mama gorda a chingue cuando cagar madre pero paner tienero putos putas de la vivida pones a el muchos a llos putos me hable todos tontas sona estoy pata se ten madre dicios por madre pre es \n"
          ]
        }
      ],
      "source": [
        "np.random.seed(7)\n",
        "sequence_3 = [\"h\", \"i\", \"j\"]\n",
        "print(\"\".join(generate_text(best_model_char, processor_char, seed=sequence_3, n_tokens=300, temperature=0.6, top_k=50)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIuJsgrAJD4j"
      },
      "source": [
        "---\n",
        "\n",
        "## 2.3) Verosimilitud de oraciones\n",
        "\n",
        "Escriba 5 ejemplos de oraciones y mídales el likelihood.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv9lss0y01ao"
      },
      "source": [
        "Ejemplos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXLhWO6u0zqG",
        "outputId": "c30e5a15-66fa-44ec-f11d-020f6a25e241"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "por eso estamos como estamos\n",
            "Log-verosimilitud: -51.607082\n",
            "------------------------------\n",
            "hola buen dia como están\n",
            "Log-verosimilitud: -57.900665\n",
            "------------------------------\n",
            "hijos de la chingada como creen\n",
            "Log-verosimilitud: -48.82113\n",
            "------------------------------\n",
            "vas a ver hijo de tu\n",
            "Log-verosimilitud: -45.066654\n",
            "------------------------------\n",
            "maldito clima hace un chingo de calor\n",
            "Log-verosimilitud: -66.77584\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    list(\"por eso estamos como estamos\"),\n",
        "    list(\"hola buen dia como están\"),\n",
        "    list(\"hijos de la chingada como creen\"),\n",
        "    list(\"vas a ver hijo de tu\"),\n",
        "    list(\"maldito clima hace un chingo de calor\")\n",
        "]\n",
        "for sent in sentences:\n",
        "    result = log_likelihood(best_model_char, sent, processor_char)\n",
        "    print(\"\".join(sent))\n",
        "    print(\"Log-verosimilitud:\", result)\n",
        "    print(\"-\"*30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCXlxulwJKc8"
      },
      "source": [
        "---\n",
        "\n",
        "## 2.4) Estructura morfológica\n",
        "\n",
        "Escriba un ejemplo de estructura morfológica (permutaciones con caracteres) similar al de estructura sintáctica del profesor con 5 o más caracteres de su gusto (e.g., \"ando\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-NP-FyQ07qW"
      },
      "source": [
        "Dada una palabra (secuencia de caracteres), calculamos la log-verosimilitud de todas las permutaciones de sus caracteres. Después se muestran las 5 permutaciones de mayor log-verosimilitud, y por lo tanto de mayor verosimilitud, i.e., las que tienen mayor estructura morgológica. También se muestran las 5 permutaciones de menor verosimilitud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "feXriHx_1P6u"
      },
      "outputs": [],
      "source": [
        "sequence = list(\"saludos\")\n",
        "\n",
        "log_likelihood_perms = [(perm, log_likelihood(best_model_char, perm, processor_char)) for perm in permutations(sequence)]\n",
        "log_likelihood_perms.sort(key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUOhoOR31dSL"
      },
      "source": [
        "Permutaciones con mayor log-verosimilitud:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5DQdcKE1feH",
        "outputId": "474fb479-9495-4ba9-a2ce-b3d2ea6473a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saludos\n",
            "Log-verosimilitud: -22.508583\n",
            "------------------------------\n",
            "saludos\n",
            "Log-verosimilitud: -22.508583\n",
            "------------------------------\n",
            "saludso\n",
            "Log-verosimilitud: -23.867504\n",
            "------------------------------\n",
            "saludso\n",
            "Log-verosimilitud: -23.867504\n",
            "------------------------------\n",
            "lusados\n",
            "Log-verosimilitud: -25.414679\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "for perm, log_lh in log_likelihood_perms[:5]:\n",
        "    print(\"\".join(perm))\n",
        "    print(\"Log-verosimilitud:\", log_lh)\n",
        "    print(\"-\"*30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlgWKQG91hwb"
      },
      "source": [
        "Permutaciones con menor log-verosimilitud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhp900Eb1ktW",
        "outputId": "ef0b51d1-d5a7-42a5-b6d0-fb8d06c5d7cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "slsuaod\n",
            "Log-verosimilitud: -67.943954\n",
            "------------------------------\n",
            "saodlsu\n",
            "Log-verosimilitud: -69.886406\n",
            "------------------------------\n",
            "saodlsu\n",
            "Log-verosimilitud: -69.886406\n",
            "------------------------------\n",
            "suaodls\n",
            "Log-verosimilitud: -72.88739\n",
            "------------------------------\n",
            "suaodls\n",
            "Log-verosimilitud: -72.88739\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "for perm, log_lh in log_likelihood_perms[-5:]:\n",
        "    print(\"\".join(perm))\n",
        "    print(\"Log-verosimilitud:\", log_lh)\n",
        "    print(\"-\"*30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muDbOzs-JR1c"
      },
      "source": [
        "---\n",
        "\n",
        "## 2.5) Perplejidad\n",
        "\n",
        "Calcule la perplejidad del modelo sobre los datos de validación.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxG1wAeK1tM4"
      },
      "source": [
        "Perplejidad del mejor modelo de caracteres sobre el conjunto de validación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fU0LLQ-11cj",
        "outputId": "57c85fbb-bb17-4410-a0a4-3469a1e4ca47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5.336686747862487"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perplexity(best_model_char, val_tk_char, processor_char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDB9_8rK15l6"
      },
      "source": [
        "---\n",
        "## 2.6) Discusión\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFWQLiC1181K"
      },
      "source": [
        "Cuando generamos texto con este modelo de lenguaje a nivel de caracteres, podemos darnos cuenta que se generan algunas palabras que sí existen, pero no hay coherencia en las oraciones. Además, el modelo logra asignarle una verosimilitud alta a la palabra \"saludos\" y también a permutaciones que sólo cambian un caracter de tal manera que se sigue entendiendo la palabra. Con este modelo se obtuvo una perplejidad muy baja, pero esto se explica ya que el vocabulario de caracteres es pequeño, por lo que predecir el siguiente caracter es una tarea menos compleja que predecir la siguiente palabra. Dado que los vocabularios de un modelo a nivel caracter y un modelo a nivel palabra tienen dos órdenes de magnitud de diferencia en cuanto a tamaño, es razonable obtener menor perplejidad con el modelo a nivel caracter."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
